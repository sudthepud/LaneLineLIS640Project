{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 2: Neural Network Baseline and Hyperparameter Optimization\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 3/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In Milestone 1 you have:\n",
    "1. **Defined a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identified three datasets** that fit your topic and justified their relevance.\n",
    "3. **Explored and visualized** the datasets to understand their structure.\n",
    "4. **Implemented a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "In Milestone 2 we will take the next step and implement a neural network baseline based on what we have learned in class! For this milestone, please use one of the datasets you picked in the last milestone. If you pick a new one, make sure to do Steps 2 - 4 again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "\n",
    "The first step is to be clear about what you want your model to predict. Is your goal a classification or a regression task? what are the input features and what are you prediction targets y? Make sure that you have a sensible choice of features and a sensible choice of prediction targets y in your dataloader.\n",
    "\n",
    "**Write down one paragraph of justification for how you set up your DataLoader below. If it makes sense to change the DataLoader from Milestone 1, describe what you changed and why:** \n",
    "We decided to completely switch our dataset to the TuSimple lane lines dataset. The reason for this is because we found that our roboflow datasets we were using before were not uniform at all in the labelling, while the TuSimple dataset provided 10000+ images with consistent labelling. So our DataLoader consists of the TuSimple dataset, where the labels are a binary mask (white or black) over an image indicating where the lane lines are. We decrease the size of each image heavily down to 512x256 to make the dataset more manageable and also implemented a amount of dataset used percentage to control how much of the 10000 images we actually use since training on all 10000 images is very time consuming. To get the TuSimple dataset formatted and set up into a DataLoader, we took inspiration from this project we found (https://github.com/IrohXu/lanenet-lane-detection-pytorch) and learned how they modified the TuSimple dataset and what transformations they applied to make the data better for training. They also implemented data shuffling so that each epoch sees the data in a different order, which we thought would be a good idea so we implemented that into our DataLoader as well. We then created a dictionary with our train and test DataLoader for ease of accessing while training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Train a Neural Network in PyTorch**\n",
    "\n",
    "We learned in class how to implement and train a feed forward neural network in pytorch. You can find reference implementations [here](https://github.com/mariru/Intro2ADL/blob/main/Week5/Week5_Lab_Example.ipynb) and [here](https://www.kaggle.com/code/girlboss/mmlm2025-pytorch-lb-0-00000). Tip: Try to implement the neural network by yourself from scratch before looking at the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "# define dataloaders: make sure to have a train, validation and a test loader\n",
    "train_dataset_file = '../../archive/TUSimple/train_set/training/train.txt'\n",
    "val_dataset_file = '../../archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "resize_height, resize_width = 256, 512\n",
    "\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample = cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "        return sample\n",
    "\n",
    "class TusimpleData(Dataset):\n",
    "    def __init__(self, dataset, n_labels=3, transform=None, target_transform=None):\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(dataset, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        self._shuffle()\n",
    "\n",
    "        # DECREASE AMOUNT OF DATA\n",
    "        purger = 0.1\n",
    "        if purger < 1.0:\n",
    "            total_size = len(self._gt_img_list)\n",
    "            subset_size = int(total_size * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        c = list(zip(self._gt_img_list, self._gt_label_binary_list))\n",
    "        random.shuffle(c)\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*c)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._gt_img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self._gt_img_list[idx])\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "        label_binary = np.zeros([label_img.shape[0], label_img.shape[1]], dtype=np.uint8)\n",
    "        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        return img, label_binary\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height)),\n",
    "])\n",
    "\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(val_loader.dataset)\n",
    "}\n",
    "\n",
    "# define the model\n",
    "class LaneLines(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaneLines, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.deconv1_binary = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2_binary = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3_binary = nn.ConvTranspose2d(32, 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        binary = self.relu(self.deconv1_binary(x))\n",
    "        binary = self.relu(self.deconv2_binary(binary))\n",
    "        binary = self.deconv3_binary(binary)\n",
    "        binary_pred = torch.argmax(binary, dim=1, keepdim=True)\n",
    "        return {\n",
    "            \"binary_seg_logits\": binary,\n",
    "            \"binary_seg_pred\": binary_pred\n",
    "        }\n",
    "\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "def compute_loss(net_output, binary_label):\n",
    "    k_binary = 10\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    binary_seg_logits = net_output[\"binary_seg_logits\"]\n",
    "    binary_loss = loss_fn(binary_seg_logits, binary_label)\n",
    "    binary_loss = binary_loss * k_binary\n",
    "    total_loss = binary_loss\n",
    "    out = net_output[\"binary_seg_pred\"]\n",
    "    return total_loss, binary_loss, out\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    for inputs, binarys in dataloader:\n",
    "        inputs = inputs.float().to(device)\n",
    "        binarys = binarys.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True): \n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, out = compute_loss(outputs, binarys)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += total_loss.item() * batch_size\n",
    "        running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "def test_loop(model, dataloader, device):\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, binarys in dataloader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            binarys = binarys.long().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, out = compute_loss(outputs, binarys)\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += total_loss.item() * batch_size\n",
    "            running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "# train the model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneLines().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "num_epochs = 100\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float(\"inf\")\n",
    "losses = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, DEVICE)\n",
    "    print(f\"Training Loss: {train_loss} | Binary Loss: {train_loss_b}\")\n",
    "    val_loss, val_loss_b = test_loop(model, val_dataloader, DEVICE)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "    losses[epoch] = val_loss\n",
    "    count = 0\n",
    "    for i in range(min(0, epoch - 5), epoch):\n",
    "        if losses[i] < val_loss:\n",
    "            count += 1\n",
    "    if count >= 4:\n",
    "        print(\"EARLY STOPPING LOSS NOT IMPROVING\")\n",
    "        break\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_wts, \"best_model.pth\")\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# test the model\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_test_data(img_path, transform):\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "def test():\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.mkdir('test_output')\n",
    "    img_path = '0001.png'\n",
    "    resize_height, resize_width = 256, 512\n",
    "    model_path = 'best_model.pth'\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    model = LaneLines()\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    inp = load_test_data(img_path, data_transform).to(DEVICE)\n",
    "    inp = torch.unsqueeze(inp, dim=0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inp)\n",
    "    input_img = Image.open(img_path)\n",
    "    input_img = input_img.resize((resize_width, resize_height))\n",
    "    input_img = np.array(input_img)\n",
    "    binary_pred = outputs['binary_seg_pred']\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "    cv2.imwrite(os.path.join('test_output', 'input_with_prediction_overlay.jpg'), overlay)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 2 continued: Try Stuff**\n",
    "\n",
    "Use your code above to try different architectures. Make sure to use early stopping! Try adding Dropout and BatchNorm, try different learning rates. How do they affect training and validation performance? \n",
    "\n",
    " **Summarize your observations in a paragraph below:**\n",
    " Since we have a large dataset, we decided to try this with only 10% of the data (1000 images). We found that deeper networks made training way more time consuming without giving us much benefit in performance of the final model, and we also found that Dropout did help with overfitting and slightly improved inference of our model, but only when we trained the model on a larger portion of the dataset, so we added Dropout. We also found that BatchNorm did not make a big difference. We found that the best learning rate was in the range of 0.001 to 0.004. We also implemented early stopping by keeping track of each epoch's loss on validation and stopping if the loss is worse than the previous 5 epochs. We hope to use the extension to mess around with different things more, since our biggest limitation has been time to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**\n",
    "\n",
    "As you can see, hyperparameter optimization can be tedious. In class we used [optuna](https://optuna.org/#code_examples) to automate the process. Your next task is to wrap your code from Step 2 into an objective which you can then optimize with optuna. Under the [code exaples](https://optuna.org/#code_examples) there is a tab *PyTorch* which should be helpful as it provides a minimal example on how to wrap PyTorch code inside an objective.\n",
    "\n",
    "**Important: Make sure the model is evaluated on a validation set, not the training data!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    pass\n",
    "\n",
    "# Create a study object\n",
    "\n",
    "# Optimize the objective function.\n",
    "\n",
    "# Print out the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b007",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**\n",
    "\n",
    "Did you find the hyperparameter search helpful? Does it help to increase the number of trials in the optimization? Note that so far we have used the simplest version of optuna which has many nice features. Can you discover more useful features by browsing the optuna website? (Hint: try pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54312a3",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n",
    "\n",
    "Now that you have found a good hyperparameter setting the validation set is no longer needed. The last step is to combine the training and validation set into a combined training set and retrain the model under the best parameter setting found. Report your final loss on your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 2 to Canvas. \n",
    "Happy Deep Learning! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
