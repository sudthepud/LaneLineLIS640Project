{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3: Model Training and Evaluation with PyTorch Lightning\n",
    "\n",
    "Welcome to Milestone 3 of LIS 640 – Introduction to Applied Deep Learning. In this milestone, you'll build upon your work from Milestones 1 and 2 by upgrading your neural network baseline to a more robust training framework using PyTorch Lightning and TensorBoard logging. You will also be exploring the advantages of different neural architectures (recurrent and convolutional neural networks) and different optimizers.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The goal of Milestone 3 is to:\n",
    "- **Explore advanced architectures:** The main goal of Milestone 3 is to strengthen your knowledge about and experience with popular neural architectures including convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
    "- **Streamline your model development:** Make sure you are working with easy-to-maintain Lightning modules.\n",
    "- **Enhance experiment tracking:** Integrate TensorBoard to log and visualize training metrics, making it easier to monitor performance and debug issues.\n",
    "- **Investigate optimizer effects:** Experiment with different optimizers (such as Adam, SGD, and RMSprop) to understand their impact on model training and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Benchmarking Feedforward NN vs. RNN on Sequence Data\n",
    "\n",
    "In this step, you'll compare the performance of a Recurrent Neural Network (RNN) against a Feedforward Neural Network (FFNN) on a dataset that contains sequential data. **For this exercise, you must use PyTorch Lightning to build your models and manage the training loop, as well as TensorBoard for logging and visualizing your training metrics.**\n",
    "\n",
    "### A. Choose Your Dataset\n",
    "\n",
    "- **Option 1:**  \n",
    "  Use one of the datasets from Milestone 1 **if it contains sequence data**.  \n",
    "  *For example, if your dataset involves time series, text, or any ordered data, it qualifies for this comparison.* In that case you have already done part B and can skip on to part C.\n",
    "  \n",
    "\n",
    "- **Option 2:**  \n",
    "  If your Milestone 1 dataset does not include sequence data, search online for and download a dataset that features sequential information (e.g., time series forecasting, text classification, sensor data, etc.). Take inspiration from previous milestones on how to do part B (Data Preparation) for your new dataset.\n",
    "\n",
    "\n",
    "\n",
    "### B. Data Preparation\n",
    "\n",
    "1. **Create a Custom Dataset Class:**  \n",
    "   - Implement a PyTorch `Dataset` class that loads your sequence data.\n",
    "   - Include any necessary preprocessing steps (e.g., normalization, tokenization, padding for sequences).\n",
    "   - Ensure that your `__getitem__` method returns the data in a format suitable for your models.\n",
    "\n",
    "2. **Build DataLoaders:**  \n",
    "   - Use `torch.utils.data.DataLoader` to create train, validation, and test loaders.\n",
    "   - Choose appropriate batch sizes and shuffling to ensure effective training.\n",
    "\n",
    "### C. Model Implementation with PyTorch Lightning\n",
    "\n",
    "*Reuse implementations from Milestone 2 if that makes sense. The key difference now is that you should implement your models as PyTorch Lightning modules to take advantage of the built-in training loop and logging features.*\n",
    "\n",
    "1. **Feedforward Neural Network (FFNN):**  \n",
    "   - Implement a baseline feedforward network that treats the sequence data as independent features (e.g., by flattening the sequence).\n",
    "   - Keep the architecture simple to establish a baseline for comparison.\n",
    "\n",
    "2. **Recurrent Neural Network (RNN):**  \n",
    "   - Implement an RNN model (using LSTM or GRU) to handle the sequential nature of the data.\n",
    "   - Ensure that your model processes the sequence appropriately (e.g., using the final hidden state or an attention mechanism for prediction).\n",
    "\n",
    "*Remember to use the PyTorch Lightning `Trainer` for model training, and configure the module to log metrics to TensorBoard.*\n",
    "\n",
    "### D. Benchmarking and Evaluation\n",
    "\n",
    "1. **Training Both Models:**  \n",
    "   - Train both the FFNN and the RNN on your chosen dataset using similar training settings (e.g., number of epochs, learning rate, optimizer) to ensure a fair comparison.\n",
    "   - Use PyTorch Lightning’s `Trainer` to manage the training process.\n",
    "\n",
    "2. **Logging and Evaluation Metrics:**  \n",
    "   - Leverage TensorBoard logging to visualize training and validation metrics in real-time.\n",
    "   - Compare the performance of both models using metrics such as loss, accuracy, or any task-specific metric.\n",
    "   - Optionally, record additional statistics like training time or convergence behavior.\n",
    "\n",
    "3. **Document Your Findings:**  \n",
    "   - Summarize the dataset and preprocessing steps.\n",
    "   - Describe the architectures used for the FFNN and RNN.\n",
    "   - Provide a comparative analysis discussing which model performed better and why that might be the case.\n",
    "   - Include TensorBoard screenshots or logged results to support your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A - Option 1 - As our lane line dataset from milestone 1 contains images, we are going to use the same dataset for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_height, resize_width = 256, 512\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "class TusimpleData(Dataset):\n",
    "    def __init__(self, dataset_file, n_labels=3, transform=None, target_transform=None, training=True, optuna=False):\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(dataset_file, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        self._shuffle()\n",
    "\n",
    "        purger = 0.5\n",
    "        if purger < 1.0 and training:\n",
    "            subset_size = int(len(self._gt_img_list) * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        zipped = list(zip(self._gt_img_list, self._gt_label_binary_list))\n",
    "        random.shuffle(zipped)\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*zipped)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._gt_img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self._gt_img_list[idx])\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "\n",
    "        label_binary = np.zeros((label_img.shape[0], label_img.shape[1]), dtype=np.uint8)\n",
    "        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        label_binary = torch.from_numpy(label_binary).long()\n",
    "        return img, label_binary\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "\n",
    "class TusimpleSeqData(Dataset):\n",
    "    def __init__(self, dataset_file, transform=None, target_transform=None, training=True, seq_len=50):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        img_list, lbl_list = [], []\n",
    "        with open(dataset_file, 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                if len(tokens) < 2:\n",
    "                    continue\n",
    "                img_list.append(tokens[0])\n",
    "                lbl_list.append(tokens[1])\n",
    "\n",
    "        if training:\n",
    "            pairs = list(zip(img_list, lbl_list))\n",
    "            random.shuffle(pairs)\n",
    "            img_list, lbl_list = zip(*pairs)\n",
    "\n",
    "        purger = 0.5\n",
    "        if purger < 1.0 and training:\n",
    "            keep = int(len(img_list) * purger)\n",
    "            img_list, lbl_list = img_list[:keep], lbl_list[:keep]\n",
    "\n",
    "        self._imgs = list(img_list)\n",
    "        self._lbls = list(lbl_list)\n",
    "\n",
    "        total_frames = len(self._imgs)\n",
    "        total_seqs = total_frames // self.seq_len\n",
    "        self.sequences = [\n",
    "            list(range(i * self.seq_len, (i + 1) * self.seq_len))\n",
    "            for i in range(total_seqs)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.sequences[idx]\n",
    "        imgs, labels = [], []\n",
    "\n",
    "        for i in indices:\n",
    "            img = Image.open(self._imgs[i]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            lbl_img = cv2.imread(self._lbls[i], cv2.IMREAD_COLOR)\n",
    "            if self.target_transform:\n",
    "                lbl_img = self.target_transform(lbl_img) \n",
    "            \n",
    "            mask = (lbl_img[:, :, :] != [0, 0, 0]).all(axis=2)\n",
    "            binary = np.zeros((lbl_img.shape[0], lbl_img.shape[1]), dtype=np.uint8)\n",
    "            binary[mask] = 1\n",
    "            binary = torch.from_numpy(binary).long()\n",
    "\n",
    "            imgs.append(img)\n",
    "            labels.append(binary)\n",
    "\n",
    "        imgs_seq = torch.stack(imgs, dim=0)\n",
    "        labels_seq = torch.stack(labels, dim=0)\n",
    "        return imgs_seq, labels_seq\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C - FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "ds_factor = 4\n",
    "small_h = resize_height // ds_factor\n",
    "small_w = resize_width  // ds_factor\n",
    "\n",
    "class LaneLinesFNN(nn.Module):\n",
    "    def __init__(self, hidden1=512, hidden2=128):\n",
    "        super().__init__()\n",
    "        self.pool    = nn.MaxPool2d(kernel_size=ds_factor)\n",
    "        self.flatten = nn.Flatten()\n",
    "        reduced_dim = 3 * small_h * small_w\n",
    "        self.fc1 = nn.Linear(reduced_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 2 * small_h * small_w)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        logits = logits.view(-1, 2, small_h, small_w)\n",
    "        logits = F.interpolate(logits, size=(resize_height, resize_width), mode='bilinear', align_corners=False)\n",
    "        pred = logits.argmax(dim=1, keepdim=True)\n",
    "        return {\"binary_seg_logits\": logits, \"binary_seg_pred\": pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSegLightningFNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesFNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fnn(model_ckpt_path):\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.makedirs('test_output')\n",
    "\n",
    "    img_path = '0001.png'\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LaneSegLightningFNN.load_from_checkpoint(model_ckpt_path)\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    inp = Image.open(img_path)\n",
    "    input_tensor = transform(inp).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    binary_logits = output['binary_seg_logits']\n",
    "    binary_pred = output['binary_seg_pred']\n",
    "    binary_logits_np = binary_logits.detach().cpu().numpy()\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "\n",
    "    input_img = np.array(inp.resize((resize_width, resize_height)))\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "\n",
    "    cv2.imwrite(\"test_output/input.jpg\", input_img)\n",
    "    cv2.imwrite(\"test_output/binary_prediction.jpg\", binary_pred_np[0, 0] * 255)\n",
    "    cv2.imwrite(\"test_output/input_with_prediction_overlay.jpg\", overlay)\n",
    "\n",
    "    for i in range(binary_logits_np.shape[1]):\n",
    "        logits = binary_logits_np[0, i, :, :]\n",
    "        logits_norm = cv2.normalize(logits, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        cv2.imwrite(f\"test_output/binary_logits_channel_{i}.jpg\", logits_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.Gates = nn.Conv2d(\n",
    "            in_channels=input_channels + hidden_channels,\n",
    "            out_channels=4 * hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        h_cur, c_cur = state\n",
    "        combined = torch.cat([x, h_cur], dim=1)\n",
    "        gates = self.Gates(combined)\n",
    "        i, f, o, g = gates.chunk(4, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.cell = ConvLSTMCell(input_channels, hidden_channels, kernel_size, padding)\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "    def forward(self, seq):\n",
    "        B, T, C, H, W = seq.size()\n",
    "        device = seq.device\n",
    "        h = torch.zeros(B, self.hidden_channels, H, W, device=device)\n",
    "        c = torch.zeros(B, self.hidden_channels, H, W, device=device)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            h, c = self.cell(seq[:, t], (h, c))\n",
    "            outputs.append(h)\n",
    "        return torch.stack(outputs, dim=1)  \n",
    "    \n",
    "class LaneLinesRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv_lstm = ConvLSTM(input_channels=32, hidden_channels=32)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(16,  2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        B, T, C, H, W = x_seq.size()\n",
    "        feats = []\n",
    "        # encode each frame\n",
    "        for t in range(T):\n",
    "            x = self.relu(self.conv1(x_seq[:, t]))\n",
    "            x = self.relu(self.conv2(x))\n",
    "            feats.append(x)\n",
    "        feats = torch.stack(feats, dim=1)\n",
    "\n",
    "        h_seq = self.conv_lstm(feats)\n",
    "\n",
    "        logits_seq, preds_seq = [], []\n",
    "        for t in range(T):\n",
    "            h = h_seq[:, t]\n",
    "            d = self.relu(self.deconv1(h))\n",
    "            logits = self.deconv2(d)\n",
    "            pred   = torch.argmax(logits, dim=1, keepdim=True)\n",
    "            logits_seq.append(logits)\n",
    "            preds_seq.append(pred)\n",
    "\n",
    "        logits_seq = torch.stack(logits_seq, dim=1)\n",
    "        preds_seq  = torch.stack(preds_seq,  dim=1)\n",
    "        return {\"binary_seg_logits\": logits_seq, \"binary_seg_pred\": preds_seq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSegRNNLightning(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model   = LaneLinesRNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr      = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        B, T, C, H, W = logits.size()\n",
    "        logits = logits.view(B * T, C, H, W)\n",
    "        target = target.view(B * T, H, W)\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_ckpt_path):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LaneSegRNNLightning.load_from_checkpoint(model_ckpt_path)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_path = '0001.png'\n",
    "    inp = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transform(inp).unsqueeze(0).to(DEVICE)\n",
    "    input_seq = input_tensor.unsqueeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq)\n",
    "\n",
    "    logits    = output['binary_seg_logits'][0, 0].cpu().numpy()\n",
    "\n",
    "    pred_mask = output['binary_seg_pred'][0, 0].squeeze(0).cpu().numpy()\n",
    "    inp_np  = np.array(inp.resize((resize_width, resize_height)))\n",
    "    overlay = inp_np.copy()\n",
    "    overlay[pred_mask > 0] = [0, 0, 255]\n",
    "\n",
    "    os.makedirs('test_output', exist_ok=True)\n",
    "    cv2.imwrite(\"test_output/input.jpg\", inp_np[:, :, ::-1])\n",
    "    cv2.imwrite(\"test_output/binary_prediction.jpg\", pred_mask * 255)\n",
    "    cv2.imwrite(\"test_output/input_with_prediction_overlay.jpg\", overlay[:, :, ::-1])\n",
    "\n",
    "    for i in range(logits.shape[0]):\n",
    "        norm = cv2.normalize(logits[i], None, 0, 255, cv2.NORM_MINMAX)\n",
    "        cv2.imwrite(f\"test_output/binary_logits_channel_{i}.jpg\", norm.astype(np.uint8))\n",
    "\n",
    "    print(\"Prediction visualization complete — see test_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D - FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 5020 samples for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | LaneLinesFNN     | 14.8 M | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "14.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.8 M    Total params\n",
      "59.050    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 4/157 [00:01<00:55,  2.75it/s, v_num=0, train_loss=4.880]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:282\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    281\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1420\u001b[0m     success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger, callbacks\u001b[38;5;241m=\u001b[39m[checkpoint], accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m test_fnn(checkpoint\u001b[38;5;241m.\u001b[39mbest_model_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:65\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     64\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     68\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningFNN()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_fnn(checkpoint.best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | LaneLinesRNN     | 83.9 K | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "83.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "83.9 K    Total params\n",
      "0.335     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 100 sequences for training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9d638721bd4b9b8931a714fc8235d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudthepud/miniconda3/envs/yv11/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4ff7d46ad14731890c8d7c012afef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c28dbd3b0e646c08def71d253d89134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a260ce274fb940cca78585983de62e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df838882624041708d8a004fbdb1bc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6464e7eaa04111a2ae3ec3f797d77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a6cfe77f954f969991cf395b9ad304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e99cd49e90d43308c3a5f79e3b89836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8b0872966340c891f6479467d808c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c443d8a4f78440c18c6316d9a8fb9736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e6cb44624a4c378662d2ad2c566bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7336a6757f1443f4a77e6750504b76eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction visualization complete — see test_output/\n"
     ]
    }
   ],
   "source": [
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file   = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleSeqData(\n",
    "    train_file,\n",
    "    transform=data_transforms['train'],\n",
    "    target_transform=target_transforms,\n",
    "    training=True,\n",
    "    seq_len=seq_len\n",
    ")\n",
    "val_ds = TusimpleSeqData(\n",
    "    val_file,\n",
    "    transform=data_transforms['val'],\n",
    "    target_transform=target_transforms,\n",
    "    training=False,\n",
    "    seq_len=seq_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} sequences for training.\")\n",
    "\n",
    "model      = LaneSegRNNLightning()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    filename=\"best_rnn_model\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Benchmarking Feedforward NN vs. CNN on Image Data\n",
    "\n",
    "In this step, you'll compare the performance of a Convolutional Neural Network (CNN) against a Feedforward Neural Network (FFNN) on an image-based dataset. **For this exercise, you must use PyTorch Lightning to implement your models and manage training, and use TensorBoard for logging and visualizing your training metrics.**\n",
    "\n",
    "### A. Choose Your Dataset\n",
    "\n",
    "- **Option 1:**  \n",
    "  Use one of the datasets from Milestone 1 **if it contains image data**.  \n",
    "  *For example, if your dataset involves images for classification, segmentation, or any visual task, it qualifies for this comparison.*\n",
    "\n",
    "- **Option 2:**  \n",
    "  If your Milestone 1 dataset does not include image data, search online for and download an image dataset (e.g., Fashion MNIST, CIFAR-10, or any domain-specific image dataset).\n",
    "\n",
    "### B. Data Preparation\n",
    "\n",
    "1. **Create a Custom Dataset Class:**  \n",
    "   - Implement a PyTorch `Dataset` class that loads your image data.\n",
    "   - Include any necessary preprocessing steps (e.g., normalization, resizing, data augmentation).\n",
    "   - Ensure that your `__getitem__` method returns the data in a format suitable for your models.\n",
    "\n",
    "2. **Build DataLoaders:**  \n",
    "   - Use `torch.utils.data.DataLoader` to create train, validation, and test loaders.\n",
    "   - Choose appropriate batch sizes and apply shuffling to ensure effective training.\n",
    "\n",
    "### C. Model Implementation with PyTorch Lightning\n",
    "\n",
    "*Reuse or adapt implementations from Milestone 2 as needed. The key requirement is to implement your models as PyTorch Lightning modules to take advantage of the built-in training loop and logging features.*\n",
    "\n",
    "1. **Feedforward Neural Network (FFNN):**  \n",
    "   - Implement a baseline FFNN that treats image data as a flat vector (i.e., by flattening the image).\n",
    "   - Keep the architecture simple to serve as a baseline for comparison.\n",
    "\n",
    "2. **Convolutional Neural Network (CNN):**  \n",
    "   - Implement a CNN architecture that leverages convolutional layers to capture spatial hierarchies in the image data.\n",
    "   - Typical layers might include convolution, activation (ReLU), pooling, and fully connected layers.\n",
    "   - Ensure that your model architecture is designed to process image data effectively.\n",
    "\n",
    "*Remember to use the PyTorch Lightning `Trainer` for training and to configure your Lightning module to log metrics to TensorBoard.*\n",
    "\n",
    "### D. Benchmarking and Evaluation\n",
    "\n",
    "1. **Training Both Models:**  \n",
    "   - Train both the FFNN and the CNN on your chosen dataset using similar training settings (e.g., number of epochs, learning rate, optimizer) to ensure a fair comparison.\n",
    "   - Use PyTorch Lightning’s `Trainer` to manage the training process.\n",
    "\n",
    "2. **Logging and Evaluation Metrics:**  \n",
    "   - Leverage TensorBoard to log and visualize training and validation metrics in real-time.\n",
    "   - Compare the performance of both models using metrics such as loss, accuracy, or any task-specific evaluation metric.\n",
    "   - Optionally, record additional details like training time and convergence behavior.\n",
    "\n",
    "3. **Document Your Findings:**  \n",
    "   - Summarize the dataset and preprocessing steps.\n",
    "   - Describe the architectures used for both the FFNN and the CNN.\n",
    "   - Provide a comparative analysis discussing which model performed better and why, supported by TensorBoard screenshots or logged results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A - Choose your dataset Class\n",
    "\n",
    "Option 1 - As our lane line dataset from milestone 1 contains images, we are going to use the same dataset for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B - Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_height, resize_width = 256, 512\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "class TusimpleData(Dataset):\n",
    "    def __init__(self, dataset_file, n_labels=3, transform=None, target_transform=None, training=True, optuna=False):\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(dataset_file, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        self._shuffle()\n",
    "\n",
    "        purger = 0.5\n",
    "        if purger < 1.0 and training:\n",
    "            subset_size = int(len(self._gt_img_list) * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        zipped = list(zip(self._gt_img_list, self._gt_label_binary_list))\n",
    "        random.shuffle(zipped)\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*zipped)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._gt_img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self._gt_img_list[idx])\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "\n",
    "        label_binary = np.zeros((label_img.shape[0], label_img.shape[1]), dtype=np.uint8)\n",
    "        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        label_binary = torch.from_numpy(label_binary).long()\n",
    "        return img, label_binary\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C - FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "ds_factor = 4\n",
    "small_h = resize_height // ds_factor\n",
    "small_w = resize_width  // ds_factor\n",
    "\n",
    "class LaneLinesFNN(nn.Module):\n",
    "    def __init__(self, hidden1=512, hidden2=128):\n",
    "        super().__init__()\n",
    "        self.pool    = nn.MaxPool2d(kernel_size=ds_factor)\n",
    "        self.flatten = nn.Flatten()\n",
    "        reduced_dim = 3 * small_h * small_w\n",
    "        self.fc1 = nn.Linear(reduced_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 2 * small_h * small_w)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        logits = logits.view(-1, 2, small_h, small_w)\n",
    "        logits = F.interpolate(logits, size=(resize_height, resize_width), mode='bilinear', align_corners=False)\n",
    "        pred = logits.argmax(dim=1, keepdim=True)\n",
    "        return {\"binary_seg_logits\": logits, \"binary_seg_pred\": pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSegLightningFNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesFNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fnn(model_ckpt_path):\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.makedirs('test_output')\n",
    "\n",
    "    img_path = '0001.png'\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LaneSegLightningFNN.load_from_checkpoint(model_ckpt_path)\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    inp = Image.open(img_path)\n",
    "    input_tensor = transform(inp).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    binary_logits = output['binary_seg_logits']\n",
    "    binary_pred = output['binary_seg_pred']\n",
    "    binary_logits_np = binary_logits.detach().cpu().numpy()\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "\n",
    "    input_img = np.array(inp.resize((resize_width, resize_height)))\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "\n",
    "    cv2.imwrite(\"test_output/input.jpg\", input_img)\n",
    "    cv2.imwrite(\"test_output/binary_prediction.jpg\", binary_pred_np[0, 0] * 255)\n",
    "    cv2.imwrite(\"test_output/input_with_prediction_overlay.jpg\", overlay)\n",
    "\n",
    "    for i in range(binary_logits_np.shape[1]):\n",
    "        logits = binary_logits_np[0, i, :, :]\n",
    "        logits_norm = cv2.normalize(logits, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        cv2.imwrite(f\"test_output/binary_logits_channel_{i}.jpg\", logits_norm)\n",
    "\n",
    "    print(\"Prediction visualization complete — see test_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneLinesCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 2, 3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        logits = self.deconv3(x)\n",
    "        pred = torch.argmax(logits, dim=1, keepdim=True)\n",
    "        return {\"binary_seg_logits\": logits, \"binary_seg_pred\": pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSegLightningCNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesCNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn(model_ckpt_path):\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.makedirs('test_output')\n",
    "\n",
    "    img_path = '0001.png'\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LaneSegLightningCNN.load_from_checkpoint(model_ckpt_path)\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    inp = Image.open(img_path)\n",
    "    input_tensor = transform(inp).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    binary_logits = output['binary_seg_logits']\n",
    "    binary_pred = output['binary_seg_pred']\n",
    "    binary_logits_np = binary_logits.detach().cpu().numpy()\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "\n",
    "    input_img = np.array(inp.resize((resize_width, resize_height)))\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "\n",
    "    cv2.imwrite(\"test_output/input.jpg\", input_img)\n",
    "    cv2.imwrite(\"test_output/binary_prediction.jpg\", binary_pred_np[0, 0] * 255)\n",
    "    cv2.imwrite(\"test_output/input_with_prediction_overlay.jpg\", overlay)\n",
    "\n",
    "    for i in range(binary_logits_np.shape[1]):\n",
    "        logits = binary_logits_np[0, i, :, :]\n",
    "        logits_norm = cv2.normalize(logits, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        cv2.imwrite(f\"test_output/binary_logits_channel_{i}.jpg\", logits_norm)\n",
    "\n",
    "    print(\"Prediction visualization complete — see test_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D - FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | LaneLinesFNN     | 14.8 M | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "14.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.8 M    Total params\n",
      "59.050    Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 5020 samples for training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccc2a9bad314a0f9bfbb3728842a222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c199cc1437841a8be72d728474ce226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4015e9c7ab24be8aa56a5c471aea7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f4a1fedb44423ea0990bdcb280c15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175fd9f17b584aca978288709daec1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e29f545b5348be8b02c204cd038553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabdd16688474ea6a3429d864fb67e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad33c8869294a3694d6d7ba7708d380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b349ec0162e4b99bdbc96f459ab30d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944cadc4bc47443aa6cdf48388dec962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedb7f0c17ab46109f4149b132487f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a37a8367a2a409c9ee509b134f69a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction visualization complete — see test_output/\n"
     ]
    }
   ],
   "source": [
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningFNN()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_fnn(checkpoint.best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | LaneLinesCNN     | 186 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "186 K     Trainable params\n",
      "0         Non-trainable params\n",
      "186 K     Total params\n",
      "0.744     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 5020 samples for training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0918b38ef44e859688b36f8e17bda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1951017e9a4688bb4e714bf6a43565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c425b5d7b7d40cfa1de6e804c32d776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8091c8d3a92f4f56afbf0e65bec007df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48775b255157414f95452c425de7fd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbe8c4b25e24bdcb9208ee47cc2676f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86f698821304e6ca441cf614e41d5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0e1e005fb74869912fb7a9bfdc9666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a7aeabb0b14b0582d80c0eded93a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a8793767d34aae8a32b3778b9bcea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158d5c4b01ab4c81966114885eca06c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5738b95b12f94f5e961d3066953a174b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction visualization complete — see test_output/\n"
     ]
    }
   ],
   "source": [
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningCNN()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_cnn(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Optimizers and Analyzing Training Curves\n",
    "\n",
    "In this step, you'll experiment with different optimizers—SGD, Adam, and RMSProp—to understand how they affect model performance. You will compare their effects using evaluation metrics on held-out test data and analyze the training and validation curves logged in TensorBoard.\n",
    "\n",
    "### A. Experiment Setup\n",
    "\n",
    "1. **Maintain Consistent Training Settings:**  \n",
    "   - Use the same model architecture (whether FFNN, CNN, or RNN from Parts 1 and 2) and dataset for all experiments.\n",
    "   - Ensure that the number of epochs, batch size, learning rate, and other hyperparameters are kept constant across different optimizer runs, aside from the optimizer itself.\n",
    "\n",
    "2. **Implement Optimizer Switching:**  \n",
    "   - Modify the `configure_optimizers` method in your PyTorch Lightning module to easily switch between optimizers:\n",
    "     ```python\n",
    "     def configure_optimizers(self):\n",
    "         # Uncomment the optimizer you want to use\n",
    "         # return torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "         # return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "         # return torch.optim.RMSprop(self.parameters(), lr=1e-3)\n",
    "     ```\n",
    "   - Train your model separately with each optimizer.\n",
    "\n",
    "### B. Evaluation Metrics and Analysis\n",
    "\n",
    "1. **Held-Out Test Evaluation:**  \n",
    "   - After training, evaluate each model on a held-out test set.\n",
    "   - Record quantitative metrics such as loss, accuracy, or any other relevant task-specific metric for each optimizer.\n",
    "\n",
    "2. **TensorBoard Analysis:**  \n",
    "   - Use TensorBoard to review the training and validation curves during training.\n",
    "   - Focus on:\n",
    "     - **Convergence Behavior:** How quickly does each optimizer reduce the loss?\n",
    "     - **Stability:** Are there noticeable fluctuations or instability in the curves?\n",
    "     - **Overfitting/Underfitting:** Do you observe signs of overfitting or underfitting, and how do these behaviors differ across optimizers?\n",
    "\n",
    "### C. Document Your Findings\n",
    "\n",
    "- **Summarize Performance:**  \n",
    "  - Create a table or a brief report comparing the evaluation metrics for SGD, Adam, and RMSProp.\n",
    "- **Include Visual Evidence:**  \n",
    "  - Attach TensorBoard screenshots or summaries of the logged training/validation curves.\n",
    "- **Provide a Comparative Analysis:**  \n",
    "  - Discuss which optimizer provided the best performance on the test set.\n",
    "  - Reflect on the convergence rates and stability differences you observed.\n",
    "  - Explain potential reasons for these differences based on your results.\n",
    "\n",
    "By the end of this exercise, you will have a deeper understanding of how different optimizers affect model training dynamics and performance. This insight is essential for making informed decisions when tuning models in future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | LaneLinesCNN     | 186 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "186 K     Trainable params\n",
      "0         Non-trainable params\n",
      "186 K     Total params\n",
      "0.744     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 5020 samples for training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f698340a7d744d8a8d30c64fc8a14177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abef0af654b4e739fee43cb29ddb222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af25fae41fa94f4fb1258944b9ee101c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b2a56302e8480ba12286a326b8741b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9beeff11e04ca180dd436d49a1e521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94d52f9522847be8ced5664d65f9efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2d5a6bd66a4463aa8569eca0c7947f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71966d2dcdb24072a99dec435bf94957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b9bbc7f79845769b119acd87139247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e53bd90dc994c3293e37c9674ecb4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431125b1ab224933a1df4191fabfc89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb45b14edac464eabd8cacb5395d844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction visualization complete — see test_output/\n"
     ]
    }
   ],
   "source": [
    "class LaneSegLightningCNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesCNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        # optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningCNN()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_cnn(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | LaneLinesCNN     | 186 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "186 K     Trainable params\n",
      "0         Non-trainable params\n",
      "186 K     Total params\n",
      "0.744     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 5020 samples for training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b931fcd9d64d3da4e4bf9c1e89f6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ce2934cd8f46798de63c66f507312f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610d67cfd6a94655bbbce9b9f54ad8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b93af688ab4602b4a6b3b1021a7c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cec3a4ef75425e861dc80101319efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91e5bc64307495bac4a056fde57f53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12cd86921e546e691137c84d29a3983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025fe032e2044fcfb60967c369ca9722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde744f9551c4a83a81e079824bac578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0900ff2a29954cb59a8eb9610c350ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd519709119744339e4dbfee2434b447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9580755e0d456b8040609e9992d789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction visualization complete — see test_output/\n"
     ]
    }
   ],
   "source": [
    "class LaneSegLightningCNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesCNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        # optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningCNN()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_cnn(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | LaneLinesCNN     | 186 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "186 K     Trainable params\n",
      "0         Non-trainable params\n",
      "186 K     Total params\n",
      "0.744     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 5020 samples for training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bcd05ed6364379bbf593cfc1c57647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de68b7880abb49ca8a5366150c79c18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bfa4f892954de5bfffc3792fcfea24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4989215f31af4693981337e33722451f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5275b5b3e8854e0aa4a72083a5c48c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545bd79ad2a849f48d707ae56d89ad7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78de2e97d5e48d2bf2bfa23f78621dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464dcb42f1954c72bb4b5edc5befa2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024c9ac97069499c81bedbb560893a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147ac3817b2242e093bd6b3c715620bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2221b773c34f4b9ffd7b947461df76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8017eb550a8f4a1e8fefe90c8d5fa8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction visualization complete — see test_output/\n"
     ]
    }
   ],
   "source": [
    "class LaneSegLightningCNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesCNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningCNN()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_cnn(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "**What to Submit:**\n",
    "\n",
    "1. Your complete iPython notebook for Milestone 3 (including all code, outputs, and markdown explanations).\n",
    "2. A single PDF file that contains your entire report for the milestone, covering:\n",
    "   - Part 1: Benchmarking FFNN vs. RNN on sequence data.\n",
    "   - Part 2: (Any additional tasks, if applicable.)\n",
    "   - Part 3: Comparing optimizers and analyzing training curves.\n",
    "\n",
    "**How to Submit:**\n",
    "\n",
    "- Upload both your iPython notebook and the PDF report to Canvas.\n",
    "- Name your files clearly, for example:\n",
    "  - `YourName_Milestone3.ipynb`\n",
    "  - `YourName_Milestone3_Report.pdf`\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "- All submissions are due **4/18/21**.\n",
    "\n",
    "Happy Deep Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
