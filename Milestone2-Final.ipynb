{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 2: Neural Network Baseline and Hyperparameter Optimization\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 3/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In Milestone 1 you have:\n",
    "1. **Defined a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identified three datasets** that fit your topic and justified their relevance.\n",
    "3. **Explored and visualized** the datasets to understand their structure.\n",
    "4. **Implemented a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "In Milestone 2 we will take the next step and implement a neural network baseline based on what we have learned in class! For this milestone, please use one of the datasets you picked in the last milestone. If you pick a new one, make sure to do Steps 2 - 4 again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "\n",
    "The first step is to be clear about what you want your model to predict. Is your goal a classification or a regression task? what are the input features and what are you prediction targets y? Make sure that you have a sensible choice of features and a sensible choice of prediction targets y in your dataloader.\n",
    "\n",
    "**Write down one paragraph of justification for how you set up your DataLoader below. If it makes sense to change the DataLoader from Milestone 1, describe what you changed and why:** \n",
    "We decided to completely switch our dataset to the TuSimple lane lines dataset. The reason for this is because we found that our roboflow datasets we were using before were not uniform at all in the labelling, while the TuSimple dataset provided 10000+ images with consistent labelling. So our DataLoader consists of the TuSimple dataset, where the labels are a binary mask (white or black) over an image indicating where the lane lines are. We decrease the size of each image heavily down to 512x256 to make the dataset more manageable and also implemented a amount of dataset used percentage to control how much of the 10000 images we actually use since training on all 10000 images is very time consuming. To get the TuSimple dataset formatted and set up into a DataLoader, we took inspiration from this project we found (https://github.com/IrohXu/lanenet-lane-detection-pytorch) and learned how they modified the TuSimple dataset and what transformations they applied to make the data better for training. They also implemented data shuffling so that each epoch sees the data in a different order, which we thought would be a good idea so we implemented that into our DataLoader as well. We then created a dictionary with our train and test DataLoader for ease of accessing while training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Train a Neural Network in PyTorch**\n",
    "\n",
    "We learned in class how to implement and train a feed forward neural network in pytorch. You can find reference implementations [here](https://github.com/mariru/Intro2ADL/blob/main/Week5/Week5_Lab_Example.ipynb) and [here](https://www.kaggle.com/code/girlboss/mmlm2025-pytorch-lb-0-00000). Tip: Try to implement the neural network by yourself from scratch before looking at the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "# define dataloaders: make sure to have a train, validation and a test loader\n",
    "train_dataset_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_dataset_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "resize_height, resize_width = 256, 512\n",
    "\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample = cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "        return sample\n",
    "\n",
    "class TusimpleData(Dataset):\n",
    "    def __init__(self, dataset, n_labels=3, transform=None, target_transform=None, training=True, optuna=False):\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(dataset, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        self._shuffle()\n",
    "\n",
    "        # DECREASE AMOUNT OF DATA\n",
    "        purger = 0.2\n",
    "        if optuna:\n",
    "            purger = 0.01\n",
    "        if purger < 1.0 and training:\n",
    "            total_size = len(self._gt_img_list)\n",
    "            subset_size = int(total_size * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        c = list(zip(self._gt_img_list, self._gt_label_binary_list))\n",
    "        random.shuffle(c)\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*c)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._gt_img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self._gt_img_list[idx])\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "        label_binary = np.zeros([label_img.shape[0], label_img.shape[1]], dtype=np.uint8)\n",
    "        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        return img, label_binary\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height)),\n",
    "])\n",
    "\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(val_loader.dataset)\n",
    "}\n",
    "\n",
    "# define the model\n",
    "class LaneLines(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaneLines, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.deconv1_binary = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2_binary = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3_binary = nn.ConvTranspose2d(32, 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        binary = self.relu(self.deconv1_binary(x))\n",
    "        binary = self.relu(self.deconv2_binary(binary))\n",
    "        binary = self.deconv3_binary(binary)\n",
    "        binary_pred = torch.argmax(binary, dim=1, keepdim=True)\n",
    "        return {\n",
    "            \"binary_seg_logits\": binary,\n",
    "            \"binary_seg_pred\": binary_pred\n",
    "        }\n",
    "\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "def compute_loss(net_output, binary_label):\n",
    "    k_binary = 10\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    binary_seg_logits = net_output[\"binary_seg_logits\"]\n",
    "    binary_loss = loss_fn(binary_seg_logits, binary_label)\n",
    "    binary_loss = binary_loss * k_binary\n",
    "    total_loss = binary_loss\n",
    "    out = net_output[\"binary_seg_pred\"]\n",
    "    return total_loss, binary_loss, out\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    for inputs, binarys in dataloader:\n",
    "        inputs = inputs.float().to(device)\n",
    "        binarys = binarys.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True): \n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, out = compute_loss(outputs, binarys)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += total_loss.item() * batch_size\n",
    "        running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "def test_loop(model, dataloader, device):\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, binarys in dataloader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            binarys = binarys.long().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, out = compute_loss(outputs, binarys)\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += total_loss.item() * batch_size\n",
    "            running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "\n",
    "# Commnted out training loop as it takes too long to run and repeating this in the last step\n",
    "\n",
    "# train the model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneLines().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "num_epochs = 100\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float(\"inf\")\n",
    "losses = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, DEVICE)\n",
    "    print(f\"Training Loss: {train_loss} | Binary Loss: {train_loss_b}\")\n",
    "    val_loss, val_loss_b = test_loop(model, val_dataloader, DEVICE)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "    losses[epoch] = val_loss\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_wts, \"best_model.pth\")\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# test the model\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_test_data(img_path, transform):\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "def test():\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.mkdir('test_output')\n",
    "    img_path = '0001.png'\n",
    "    resize_height, resize_width = 256, 512\n",
    "    model_path = 'best_model.pth'\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    model = LaneLines()\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    inp = load_test_data(img_path, data_transform).to(DEVICE)\n",
    "    inp = torch.unsqueeze(inp, dim=0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inp)\n",
    "    input_img = Image.open(img_path)\n",
    "    input_img = input_img.resize((resize_width, resize_height))\n",
    "    input_img = np.array(input_img)\n",
    "    binary_pred = outputs['binary_seg_pred']\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "    cv2.imwrite(os.path.join('test_output', 'input_with_prediction_overlay.jpg'), overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 2 continued: Try Stuff**\n",
    "\n",
    "Use your code above to try different architectures. Make sure to use early stopping! Try adding Dropout and BatchNorm, try different learning rates. How do they affect training and validation performance? \n",
    "\n",
    " **Summarize your observations in a paragraph below:**\n",
    " Since we have a large dataset, we decided to try this with only 10% of the data (1000 images). We found that deeper networks made training way more time consuming without giving us much benefit in performance of the final model, and we also found that Dropout did help with overfitting and slightly improved inference of our model, but only when we trained the model on a larger portion of the dataset, so we added Dropout. We also found that BatchNorm did not make a big difference. We found that the best learning rate was in the range of 0.001 to 0.004. We also implemented early stopping by keeping track of each epoch's loss on validation and stopping if the loss is worse than the previous 5 epochs. We hope to use the extension to mess around with different things more, since our biggest limitation has been time to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**\n",
    "\n",
    "As you can see, hyperparameter optimization can be tedious. In class we used [optuna](https://optuna.org/#code_examples) to automate the process. Your next task is to wrap your code from Step 2 into an objective which you can then optimize with optuna. Under the [code exaples](https://optuna.org/#code_examples) there is a tab *PyTorch* which should be helpful as it provides a minimal example on how to wrap PyTorch code inside an objective.\n",
    "\n",
    "**Important: Make sure the model is evaluated on a validation set, not the training data!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584d1593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sriram/anaconda3/envs/yolov10/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-04 10:31:01,595] A new study created in memory with name: no-name-0d616d61-d37e-4f4a-839e-b2af6c9d7c85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_681933/3962289433.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Training Loss: 853.4618 | Binary Loss: 853.4618\n",
      "Validation Loss: 6357.5110 | Binary Loss: 6357.5110\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-04 10:31:27,755] Trial 0 failed with parameters: {'lr': 0.00513443621337721, 'batch_size': 64, 'optimizer': 'SGD'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sriram/anaconda3/envs/yolov10/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_681933/3962289433.py\", line 41, in objective\n",
      "    train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, device)\n",
      "  File \"/tmp/ipykernel_681933/1764026698.py\", line 164, in train_loop\n",
      "    for inputs, binarys in dataloader:\n",
      "  File \"/home/sriram/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/sriram/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/sriram/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/sriram/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/tmp/ipykernel_681933/1764026698.py\", line 79, in __getitem__\n",
      "    label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-04 10:31:27,757] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_startup_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Optimize the objective function\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Print out the best parameters\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     train_loss, train_loss_b \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Binary Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss_b\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     val_loss, val_loss_b \u001b[38;5;241m=\u001b[39m test_loop(model, val_dataloader, device)\n",
      "Cell \u001b[0;32mIn[1], line 164\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m    161\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    162\u001b[0m running_loss_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, binarys \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    165\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    166\u001b[0m     binarys \u001b[38;5;241m=\u001b[39m binarys\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mTusimpleData.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     78\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gt_img_list[idx])\n\u001b[0;32m---> 79\u001b[0m     label_img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gt_label_binary_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_COLOR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     81\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna.exceptions\n",
    "\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=True, optuna=True)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "\n",
    "# Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LaneLines().to(device)\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    # loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "    num_epochs = 15\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"Training Loss: {train_loss:.4f} | Binary Loss: {train_loss_b:.4f}\")\n",
    "\n",
    "        val_loss, val_loss_b = test_loop(model, val_dataloader, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "\n",
    "# Optimize the objective function\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print out the best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b007",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**\n",
    "\n",
    "The hyperparameter search was somewhat helpful, but we think we weren’t able to harness a lot of the potential benefits due to the significant training times when running the Optuna trials – even with only using a small percentage (5-10%) of the data. While increasing the number of trials did yield hyperparameters that improved results, we could not increase the number of trials by a significant amount. Additionally, we found the pruning feature on the Optuna website and found it to be useful in cutting off less promising trials early, which allowed us to bump up the number of trials. We plan to look into potentially using cloud compute so we can run more, longer trials for the hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54312a3",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n",
    "\n",
    "Now that you have found a good hyperparameter setting the validation set is no longer needed. The last step is to combine the training and validation set into a combined training set and retrain the model under the best parameter setting found. Report your final loss on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_lr = study.best_params['lr']\n",
    "optuna_batch_size = study.best_params['batch_size']\n",
    "optuna_optimizer = study.best_params['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Training Loss: 1879.4857931137085 | Binary Loss: 1879.4857931137085\n",
      "Validation Loss: 604.4004 | Binary Loss: 604.4004\n",
      "Epoch 2/100\n",
      "Training Loss: 1405.152413368225 | Binary Loss: 1405.152413368225\n",
      "Validation Loss: 564.0801 | Binary Loss: 564.0801\n",
      "Epoch 3/100\n",
      "Training Loss: 1327.1682257652283 | Binary Loss: 1327.1682257652283\n",
      "Validation Loss: 531.2248 | Binary Loss: 531.2248\n",
      "Epoch 4/100\n",
      "Training Loss: 1260.9797005653381 | Binary Loss: 1260.9797005653381\n",
      "Validation Loss: 508.2955 | Binary Loss: 508.2955\n",
      "Epoch 5/100\n",
      "Training Loss: 1218.2152633666992 | Binary Loss: 1218.2152633666992\n",
      "Validation Loss: 480.7238 | Binary Loss: 480.7238\n",
      "Epoch 6/100\n",
      "Training Loss: 1162.5182104110718 | Binary Loss: 1162.5182104110718\n",
      "Validation Loss: 469.5759 | Binary Loss: 469.5759\n",
      "Epoch 7/100\n",
      "Training Loss: 1126.847858428955 | Binary Loss: 1126.847858428955\n",
      "Validation Loss: 458.5229 | Binary Loss: 458.5229\n",
      "Epoch 8/100\n",
      "Training Loss: 1112.384331703186 | Binary Loss: 1112.384331703186\n",
      "Validation Loss: 444.9471 | Binary Loss: 444.9471\n",
      "Epoch 9/100\n",
      "Training Loss: 1084.9497809410095 | Binary Loss: 1084.9497809410095\n",
      "Validation Loss: 442.7101 | Binary Loss: 442.7101\n",
      "Epoch 10/100\n",
      "Training Loss: 1064.9062461853027 | Binary Loss: 1064.9062461853027\n",
      "Validation Loss: 430.7434 | Binary Loss: 430.7434\n",
      "Epoch 11/100\n",
      "Training Loss: 1016.6384761333466 | Binary Loss: 1016.6384761333466\n",
      "Validation Loss: 418.5609 | Binary Loss: 418.5609\n",
      "Epoch 12/100\n",
      "Training Loss: 1003.7713389396667 | Binary Loss: 1003.7713389396667\n",
      "Validation Loss: 416.7241 | Binary Loss: 416.7241\n",
      "Epoch 13/100\n",
      "Training Loss: 999.085883140564 | Binary Loss: 999.085883140564\n",
      "Validation Loss: 412.5164 | Binary Loss: 412.5164\n",
      "Epoch 14/100\n",
      "Training Loss: 995.008261680603 | Binary Loss: 995.008261680603\n",
      "Validation Loss: 412.2918 | Binary Loss: 412.2918\n",
      "Epoch 15/100\n",
      "Training Loss: 991.2496304512024 | Binary Loss: 991.2496304512024\n",
      "Validation Loss: 409.4670 | Binary Loss: 409.4670\n",
      "Epoch 16/100\n",
      "Training Loss: 989.2933821678162 | Binary Loss: 989.2933821678162\n",
      "Validation Loss: 409.9065 | Binary Loss: 409.9065\n",
      "Epoch 17/100\n",
      "Training Loss: 985.0149574279785 | Binary Loss: 985.0149574279785\n",
      "Validation Loss: 406.2409 | Binary Loss: 406.2409\n",
      "Epoch 18/100\n",
      "Training Loss: 981.0783159732819 | Binary Loss: 981.0783159732819\n",
      "Validation Loss: 406.8029 | Binary Loss: 406.8029\n",
      "Epoch 19/100\n",
      "Training Loss: 979.2248153686523 | Binary Loss: 979.2248153686523\n",
      "Validation Loss: 405.2617 | Binary Loss: 405.2617\n",
      "Epoch 20/100\n",
      "Training Loss: 975.4011993408203 | Binary Loss: 975.4011993408203\n",
      "Validation Loss: 404.7172 | Binary Loss: 404.7172\n",
      "Epoch 21/100\n",
      "Training Loss: 966.3694996833801 | Binary Loss: 966.3694996833801\n",
      "Validation Loss: 400.9798 | Binary Loss: 400.9798\n",
      "Epoch 22/100\n",
      "Training Loss: 964.5091547966003 | Binary Loss: 964.5091547966003\n",
      "Validation Loss: 400.7989 | Binary Loss: 400.7989\n",
      "Epoch 23/100\n",
      "Training Loss: 963.2154703140259 | Binary Loss: 963.2154703140259\n",
      "Validation Loss: 400.5063 | Binary Loss: 400.5063\n",
      "Epoch 24/100\n",
      "Training Loss: 962.7980380058289 | Binary Loss: 962.7980380058289\n",
      "Validation Loss: 400.4534 | Binary Loss: 400.4534\n",
      "Epoch 25/100\n",
      "Training Loss: 962.6347343921661 | Binary Loss: 962.6347343921661\n",
      "Validation Loss: 399.9430 | Binary Loss: 399.9430\n",
      "Epoch 26/100\n",
      "Training Loss: 962.1467657089233 | Binary Loss: 962.1467657089233\n",
      "Validation Loss: 399.6276 | Binary Loss: 399.6276\n",
      "Epoch 27/100\n",
      "Training Loss: 960.7019424438477 | Binary Loss: 960.7019424438477\n",
      "Validation Loss: 399.6956 | Binary Loss: 399.6956\n",
      "Epoch 28/100\n",
      "Training Loss: 960.8403100967407 | Binary Loss: 960.8403100967407\n",
      "Validation Loss: 399.5121 | Binary Loss: 399.5121\n",
      "Epoch 29/100\n",
      "Training Loss: 959.9081091880798 | Binary Loss: 959.9081091880798\n",
      "Validation Loss: 399.0486 | Binary Loss: 399.0486\n",
      "Epoch 30/100\n",
      "Training Loss: 959.8144402503967 | Binary Loss: 959.8144402503967\n",
      "Validation Loss: 398.8705 | Binary Loss: 398.8705\n",
      "Epoch 31/100\n",
      "Training Loss: 958.4779953956604 | Binary Loss: 958.4779953956604\n",
      "Validation Loss: 398.8487 | Binary Loss: 398.8487\n",
      "Epoch 32/100\n",
      "Training Loss: 959.3998484611511 | Binary Loss: 959.3998484611511\n",
      "Validation Loss: 398.8638 | Binary Loss: 398.8638\n",
      "Epoch 33/100\n",
      "Training Loss: 957.285406589508 | Binary Loss: 957.285406589508\n",
      "Validation Loss: 398.8664 | Binary Loss: 398.8664\n",
      "Epoch 34/100\n",
      "Training Loss: 959.2408502101898 | Binary Loss: 959.2408502101898\n",
      "Validation Loss: 398.7724 | Binary Loss: 398.7724\n",
      "Epoch 35/100\n",
      "Training Loss: 958.3968505859375 | Binary Loss: 958.3968505859375\n",
      "Validation Loss: 398.8718 | Binary Loss: 398.8718\n",
      "Epoch 36/100\n",
      "Training Loss: 958.0125851631165 | Binary Loss: 958.0125851631165\n",
      "Validation Loss: 398.7863 | Binary Loss: 398.7863\n",
      "Epoch 37/100\n",
      "Training Loss: 958.1441304683685 | Binary Loss: 958.1441304683685\n",
      "Validation Loss: 398.7715 | Binary Loss: 398.7715\n",
      "Epoch 38/100\n",
      "Training Loss: 957.814254283905 | Binary Loss: 957.814254283905\n",
      "Validation Loss: 398.7160 | Binary Loss: 398.7160\n",
      "Epoch 39/100\n",
      "Training Loss: 958.8384871482849 | Binary Loss: 958.8384871482849\n",
      "Validation Loss: 398.7369 | Binary Loss: 398.7369\n",
      "Epoch 40/100\n",
      "Training Loss: 957.6619548797607 | Binary Loss: 957.6619548797607\n",
      "Validation Loss: 398.6734 | Binary Loss: 398.6734\n",
      "Epoch 41/100\n",
      "Training Loss: 957.8087816238403 | Binary Loss: 957.8087816238403\n",
      "Validation Loss: 398.6796 | Binary Loss: 398.6796\n",
      "Epoch 42/100\n",
      "Training Loss: 957.6520104408264 | Binary Loss: 957.6520104408264\n",
      "Validation Loss: 398.6625 | Binary Loss: 398.6625\n",
      "Epoch 43/100\n",
      "Training Loss: 957.660101890564 | Binary Loss: 957.660101890564\n",
      "Validation Loss: 398.6719 | Binary Loss: 398.6719\n",
      "Epoch 44/100\n",
      "Training Loss: 956.8836822509766 | Binary Loss: 956.8836822509766\n",
      "Validation Loss: 398.6768 | Binary Loss: 398.6768\n",
      "Epoch 45/100\n",
      "Training Loss: 957.3750371932983 | Binary Loss: 957.3750371932983\n",
      "Validation Loss: 398.6681 | Binary Loss: 398.6681\n",
      "Epoch 46/100\n",
      "Training Loss: 958.0739903450012 | Binary Loss: 958.0739903450012\n",
      "Validation Loss: 398.6516 | Binary Loss: 398.6516\n",
      "Epoch 47/100\n",
      "Training Loss: 959.0362622737885 | Binary Loss: 959.0362622737885\n",
      "Validation Loss: 398.6620 | Binary Loss: 398.6620\n",
      "Epoch 48/100\n",
      "Training Loss: 957.6674642562866 | Binary Loss: 957.6674642562866\n",
      "Validation Loss: 398.6498 | Binary Loss: 398.6498\n",
      "Epoch 49/100\n",
      "Training Loss: 958.1682391166687 | Binary Loss: 958.1682391166687\n",
      "Validation Loss: 398.6369 | Binary Loss: 398.6369\n",
      "Epoch 50/100\n",
      "Training Loss: 958.0804724693298 | Binary Loss: 958.0804724693298\n",
      "Validation Loss: 398.6410 | Binary Loss: 398.6410\n",
      "Epoch 51/100\n",
      "Training Loss: 957.5014429092407 | Binary Loss: 957.5014429092407\n",
      "Validation Loss: 398.6401 | Binary Loss: 398.6401\n",
      "Epoch 52/100\n",
      "Training Loss: 958.4540419578552 | Binary Loss: 958.4540419578552\n",
      "Validation Loss: 398.6402 | Binary Loss: 398.6402\n",
      "Epoch 53/100\n",
      "Training Loss: 957.2646949291229 | Binary Loss: 957.2646949291229\n",
      "Validation Loss: 398.6403 | Binary Loss: 398.6403\n",
      "Epoch 54/100\n",
      "Training Loss: 957.1656546592712 | Binary Loss: 957.1656546592712\n",
      "Validation Loss: 398.6382 | Binary Loss: 398.6382\n",
      "Epoch 55/100\n",
      "Training Loss: 957.9883470535278 | Binary Loss: 957.9883470535278\n",
      "Validation Loss: 398.6383 | Binary Loss: 398.6383\n",
      "Epoch 56/100\n",
      "Training Loss: 957.5020394325256 | Binary Loss: 957.5020394325256\n",
      "Validation Loss: 398.6381 | Binary Loss: 398.6381\n",
      "Epoch 57/100\n",
      "Training Loss: 957.711757183075 | Binary Loss: 957.711757183075\n",
      "Validation Loss: 398.6375 | Binary Loss: 398.6375\n",
      "Epoch 58/100\n",
      "Training Loss: 958.5630259513855 | Binary Loss: 958.5630259513855\n",
      "Validation Loss: 398.6367 | Binary Loss: 398.6367\n",
      "Epoch 59/100\n",
      "Training Loss: 957.4458079338074 | Binary Loss: 957.4458079338074\n",
      "Validation Loss: 398.6363 | Binary Loss: 398.6363\n",
      "Epoch 60/100\n",
      "Training Loss: 957.4815058708191 | Binary Loss: 957.4815058708191\n",
      "Validation Loss: 398.6369 | Binary Loss: 398.6369\n",
      "Epoch 61/100\n",
      "Training Loss: 957.9119148254395 | Binary Loss: 957.9119148254395\n",
      "Validation Loss: 398.6370 | Binary Loss: 398.6370\n",
      "Epoch 62/100\n",
      "Training Loss: 958.5494475364685 | Binary Loss: 958.5494475364685\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 63/100\n",
      "Training Loss: 958.9163465499878 | Binary Loss: 958.9163465499878\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 64/100\n",
      "Training Loss: 958.0416457653046 | Binary Loss: 958.0416457653046\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 65/100\n",
      "Training Loss: 957.513206243515 | Binary Loss: 957.513206243515\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 66/100\n",
      "Training Loss: 958.5706076622009 | Binary Loss: 958.5706076622009\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 67/100\n",
      "Training Loss: 958.1161389350891 | Binary Loss: 958.1161389350891\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 68/100\n",
      "Training Loss: 957.927318572998 | Binary Loss: 957.927318572998\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 69/100\n",
      "Training Loss: 958.6417555809021 | Binary Loss: 958.6417555809021\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 70/100\n",
      "Training Loss: 958.1138887405396 | Binary Loss: 958.1138887405396\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 71/100\n",
      "Training Loss: 957.1654133796692 | Binary Loss: 957.1654133796692\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 72/100\n",
      "Training Loss: 957.8480274677277 | Binary Loss: 957.8480274677277\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 73/100\n",
      "Training Loss: 957.8296709060669 | Binary Loss: 957.8296709060669\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 74/100\n",
      "Training Loss: 957.2758314609528 | Binary Loss: 957.2758314609528\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 75/100\n",
      "Training Loss: 957.1763706207275 | Binary Loss: 957.1763706207275\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 76/100\n",
      "Training Loss: 958.7701427936554 | Binary Loss: 958.7701427936554\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 77/100\n",
      "Training Loss: 958.7455384731293 | Binary Loss: 958.7455384731293\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 78/100\n",
      "Training Loss: 958.5385060310364 | Binary Loss: 958.5385060310364\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 79/100\n",
      "Training Loss: 957.0862755775452 | Binary Loss: 957.0862755775452\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 80/100\n",
      "Training Loss: 957.7689290046692 | Binary Loss: 957.7689290046692\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 81/100\n",
      "Training Loss: 958.003874540329 | Binary Loss: 958.003874540329\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 82/100\n",
      "Training Loss: 957.4278607368469 | Binary Loss: 957.4278607368469\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 83/100\n",
      "Training Loss: 957.3201942443848 | Binary Loss: 957.3201942443848\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 84/100\n",
      "Training Loss: 957.4225809574127 | Binary Loss: 957.4225809574127\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 85/100\n",
      "Training Loss: 957.5094141960144 | Binary Loss: 957.5094141960144\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 86/100\n",
      "Training Loss: 957.2252860069275 | Binary Loss: 957.2252860069275\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 87/100\n",
      "Training Loss: 958.3746137619019 | Binary Loss: 958.3746137619019\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 88/100\n",
      "Training Loss: 957.4495213031769 | Binary Loss: 957.4495213031769\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 89/100\n",
      "Training Loss: 957.6788396835327 | Binary Loss: 957.6788396835327\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 90/100\n",
      "Training Loss: 957.2353591918945 | Binary Loss: 957.2353591918945\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 91/100\n",
      "Training Loss: 958.1560823917389 | Binary Loss: 958.1560823917389\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 92/100\n",
      "Training Loss: 958.1172575950623 | Binary Loss: 958.1172575950623\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 93/100\n",
      "Training Loss: 957.6524209976196 | Binary Loss: 957.6524209976196\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 94/100\n",
      "Training Loss: 957.9860935211182 | Binary Loss: 957.9860935211182\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 95/100\n",
      "Training Loss: 958.6131510734558 | Binary Loss: 958.6131510734558\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 96/100\n",
      "Training Loss: 958.2453832626343 | Binary Loss: 958.2453832626343\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 97/100\n",
      "Training Loss: 957.1972143650055 | Binary Loss: 957.1972143650055\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 98/100\n",
      "Training Loss: 957.5225486755371 | Binary Loss: 957.5225486755371\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 99/100\n",
      "Training Loss: 958.1516966819763 | Binary Loss: 958.1516966819763\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 100/100\n",
      "Training Loss: 957.3454031944275 | Binary Loss: 957.3454031944275\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneLines().to(DEVICE)\n",
    "\n",
    "if optuna_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=optuna_lr, momentum=0.9)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=optuna_lr)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=optuna_batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=optuna_batch_size, shuffle=False)\n",
    "num_epochs = 100\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float(\"inf\")\n",
    "losses = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, DEVICE)\n",
    "    print(f\"Training Loss: {train_loss} | Binary Loss: {train_loss_b}\")\n",
    "    val_loss, val_loss_b = test_loop(model, val_dataloader, DEVICE)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "    losses[epoch] = val_loss\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_wts, \"best_model.pth\")\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# test the model\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_test_data(img_path, transform):\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "def test():\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.mkdir('test_output')\n",
    "    img_path = '0001.png'\n",
    "    resize_height, resize_width = 256, 512\n",
    "    model_path = 'best_model.pth'\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    model = LaneLines()\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    inp = load_test_data(img_path, data_transform).to(DEVICE)\n",
    "    inp = torch.unsqueeze(inp, dim=0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inp)\n",
    "    input_img = Image.open(img_path)\n",
    "    input_img = input_img.resize((resize_width, resize_height))\n",
    "    input_img = np.array(input_img)\n",
    "    binary_pred = outputs['binary_seg_pred']\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "    cv2.imwrite(os.path.join('test_output', 'input_with_prediction_overlay.jpg'), overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 2 to Canvas. \n",
    "Happy Deep Learning! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
