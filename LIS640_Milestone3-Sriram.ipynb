{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3: Model Training and Evaluation with PyTorch Lightning\n",
    "\n",
    "Welcome to Milestone 3 of LIS 640 – Introduction to Applied Deep Learning. In this milestone, you'll build upon your work from Milestones 1 and 2 by upgrading your neural network baseline to a more robust training framework using PyTorch Lightning and TensorBoard logging. You will also be exploring the advantages of different neural architectures (recurrent and convolutional neural networks) and different optimizers.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The goal of Milestone 3 is to:\n",
    "- **Explore advanced architectures:** The main goal of Milestone 3 is to strengthen your knowledge about and experience with popular neural architectures including convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
    "- **Streamline your model development:** Make sure you are working with easy-to-maintain Lightning modules.\n",
    "- **Enhance experiment tracking:** Integrate TensorBoard to log and visualize training metrics, making it easier to monitor performance and debug issues.\n",
    "- **Investigate optimizer effects:** Experiment with different optimizers (such as Adam, SGD, and RMSprop) to understand their impact on model training and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Benchmarking Feedforward NN vs. RNN on Sequence Data\n",
    "\n",
    "In this step, you'll compare the performance of a Recurrent Neural Network (RNN) against a Feedforward Neural Network (FFNN) on a dataset that contains sequential data. **For this exercise, you must use PyTorch Lightning to build your models and manage the training loop, as well as TensorBoard for logging and visualizing your training metrics.**\n",
    "\n",
    "### A. Choose Your Dataset\n",
    "\n",
    "- **Option 1:**  \n",
    "  Use one of the datasets from Milestone 1 **if it contains sequence data**.  \n",
    "  *For example, if your dataset involves time series, text, or any ordered data, it qualifies for this comparison.* In that case you have already done part B and can skip on to part C.\n",
    "  \n",
    "\n",
    "- **Option 2:**  \n",
    "  If your Milestone 1 dataset does not include sequence data, search online for and download a dataset that features sequential information (e.g., time series forecasting, text classification, sensor data, etc.). Take inspiration from previous milestones on how to do part B (Data Preparation) for your new dataset.\n",
    "\n",
    "\n",
    "\n",
    "### B. Data Preparation\n",
    "\n",
    "1. **Create a Custom Dataset Class:**  \n",
    "   - Implement a PyTorch `Dataset` class that loads your sequence data.\n",
    "   - Include any necessary preprocessing steps (e.g., normalization, tokenization, padding for sequences).\n",
    "   - Ensure that your `__getitem__` method returns the data in a format suitable for your models.\n",
    "\n",
    "2. **Build DataLoaders:**  \n",
    "   - Use `torch.utils.data.DataLoader` to create train, validation, and test loaders.\n",
    "   - Choose appropriate batch sizes and shuffling to ensure effective training.\n",
    "\n",
    "### C. Model Implementation with PyTorch Lightning\n",
    "\n",
    "*Reuse implementations from Milestone 2 if that makes sense. The key difference now is that you should implement your models as PyTorch Lightning modules to take advantage of the built-in training loop and logging features.*\n",
    "\n",
    "1. **Feedforward Neural Network (FFNN):**  \n",
    "   - Implement a baseline feedforward network that treats the sequence data as independent features (e.g., by flattening the sequence).\n",
    "   - Keep the architecture simple to establish a baseline for comparison.\n",
    "\n",
    "2. **Recurrent Neural Network (RNN):**  \n",
    "   - Implement an RNN model (using LSTM or GRU) to handle the sequential nature of the data.\n",
    "   - Ensure that your model processes the sequence appropriately (e.g., using the final hidden state or an attention mechanism for prediction).\n",
    "\n",
    "*Remember to use the PyTorch Lightning `Trainer` for model training, and configure the module to log metrics to TensorBoard.*\n",
    "\n",
    "### D. Benchmarking and Evaluation\n",
    "\n",
    "1. **Training Both Models:**  \n",
    "   - Train both the FFNN and the RNN on your chosen dataset using similar training settings (e.g., number of epochs, learning rate, optimizer) to ensure a fair comparison.\n",
    "   - Use PyTorch Lightning’s `Trainer` to manage the training process.\n",
    "\n",
    "2. **Logging and Evaluation Metrics:**  \n",
    "   - Leverage TensorBoard logging to visualize training and validation metrics in real-time.\n",
    "   - Compare the performance of both models using metrics such as loss, accuracy, or any task-specific metric.\n",
    "   - Optionally, record additional statistics like training time or convergence behavior.\n",
    "\n",
    "3. **Document Your Findings:**  \n",
    "   - Summarize the dataset and preprocessing steps.\n",
    "   - Describe the architectures used for the FFNN and RNN.\n",
    "   - Provide a comparative analysis discussing which model performed better and why that might be the case.\n",
    "   - Include TensorBoard screenshots or logged results to support your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A - Option 1 - As our lane line dataset from milestone 1 contains images, we are going to use the same dataset for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_height, resize_width = 256, 512\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "class TusimpleData(Dataset):\n",
    "    def __init__(self, dataset_file, n_labels=3, transform=None, target_transform=None, training=True, optuna=False):\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(dataset_file, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        self._shuffle()\n",
    "\n",
    "        purger = 1\n",
    "        if purger < 1.0 and training:\n",
    "            subset_size = int(len(self._gt_img_list) * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        zipped = list(zip(self._gt_img_list, self._gt_label_binary_list))\n",
    "        random.shuffle(zipped)\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*zipped)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._gt_img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self._gt_img_list[idx])\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "\n",
    "        label_binary = np.zeros((label_img.shape[0], label_img.shape[1]), dtype=np.uint8)\n",
    "        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        label_binary = torch.from_numpy(label_binary).long()\n",
    "        return img, label_binary\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C - FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3 * resize_height * resize_width\n",
    "output_dim = 2 * resize_height * resize_width\n",
    "\n",
    "class LaneLinesFNN(nn.Module):\n",
    "    def __init__(self, hidden1=1024, hidden2=256):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        logits = logits.view(-1, 2, resize_height, resize_width)\n",
    "        pred = torch.argmax(logits, dim=1, keepdim=True)\n",
    "        return {\"binary_seg_logits\": logits, \"binary_seg_pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSegLightningFNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesFNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fnn(model_ckpt_path):\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.makedirs('test_output')\n",
    "\n",
    "    img_path = '0001.png'\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LaneSegLightning.load_from_checkpoint(model_ckpt_path)\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    inp = Image.open(img_path)\n",
    "    input_tensor = transform(inp).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    binary_logits = output['binary_seg_logits']\n",
    "    binary_pred = output['binary_seg_pred']\n",
    "    binary_logits_np = binary_logits.detach().cpu().numpy()\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "\n",
    "    input_img = np.array(inp.resize((resize_width, resize_height)))\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "\n",
    "    cv2.imwrite(\"test_output/input.jpg\", input_img)\n",
    "    cv2.imwrite(\"test_output/binary_prediction.jpg\", binary_pred_np[0, 0] * 255)\n",
    "    cv2.imwrite(\"test_output/input_with_prediction_overlay.jpg\", overlay)\n",
    "\n",
    "    for i in range(binary_logits_np.shape[1]):\n",
    "        logits = binary_logits_np[0, i, :, :]\n",
    "        logits_norm = cv2.normalize(logits, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        cv2.imwrite(f\"test_output/binary_logits_channel_{i}.jpg\", logits_norm)\n",
    "\n",
    "    print(\"✅ Prediction visualization complete — see test_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D - FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningFNN()\n",
    "logger = CSVLogger(\"logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_fnn(checkpoint.best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Benchmarking Feedforward NN vs. CNN on Image Data\n",
    "\n",
    "In this step, you'll compare the performance of a Convolutional Neural Network (CNN) against a Feedforward Neural Network (FFNN) on an image-based dataset. **For this exercise, you must use PyTorch Lightning to implement your models and manage training, and use TensorBoard for logging and visualizing your training metrics.**\n",
    "\n",
    "### A. Choose Your Dataset\n",
    "\n",
    "- **Option 1:**  \n",
    "  Use one of the datasets from Milestone 1 **if it contains image data**.  \n",
    "  *For example, if your dataset involves images for classification, segmentation, or any visual task, it qualifies for this comparison.*\n",
    "\n",
    "- **Option 2:**  \n",
    "  If your Milestone 1 dataset does not include image data, search online for and download an image dataset (e.g., Fashion MNIST, CIFAR-10, or any domain-specific image dataset).\n",
    "\n",
    "### B. Data Preparation\n",
    "\n",
    "1. **Create a Custom Dataset Class:**  \n",
    "   - Implement a PyTorch `Dataset` class that loads your image data.\n",
    "   - Include any necessary preprocessing steps (e.g., normalization, resizing, data augmentation).\n",
    "   - Ensure that your `__getitem__` method returns the data in a format suitable for your models.\n",
    "\n",
    "2. **Build DataLoaders:**  \n",
    "   - Use `torch.utils.data.DataLoader` to create train, validation, and test loaders.\n",
    "   - Choose appropriate batch sizes and apply shuffling to ensure effective training.\n",
    "\n",
    "### C. Model Implementation with PyTorch Lightning\n",
    "\n",
    "*Reuse or adapt implementations from Milestone 2 as needed. The key requirement is to implement your models as PyTorch Lightning modules to take advantage of the built-in training loop and logging features.*\n",
    "\n",
    "1. **Feedforward Neural Network (FFNN):**  \n",
    "   - Implement a baseline FFNN that treats image data as a flat vector (i.e., by flattening the image).\n",
    "   - Keep the architecture simple to serve as a baseline for comparison.\n",
    "\n",
    "2. **Convolutional Neural Network (CNN):**  \n",
    "   - Implement a CNN architecture that leverages convolutional layers to capture spatial hierarchies in the image data.\n",
    "   - Typical layers might include convolution, activation (ReLU), pooling, and fully connected layers.\n",
    "   - Ensure that your model architecture is designed to process image data effectively.\n",
    "\n",
    "*Remember to use the PyTorch Lightning `Trainer` for training and to configure your Lightning module to log metrics to TensorBoard.*\n",
    "\n",
    "### D. Benchmarking and Evaluation\n",
    "\n",
    "1. **Training Both Models:**  \n",
    "   - Train both the FFNN and the CNN on your chosen dataset using similar training settings (e.g., number of epochs, learning rate, optimizer) to ensure a fair comparison.\n",
    "   - Use PyTorch Lightning’s `Trainer` to manage the training process.\n",
    "\n",
    "2. **Logging and Evaluation Metrics:**  \n",
    "   - Leverage TensorBoard to log and visualize training and validation metrics in real-time.\n",
    "   - Compare the performance of both models using metrics such as loss, accuracy, or any task-specific evaluation metric.\n",
    "   - Optionally, record additional details like training time and convergence behavior.\n",
    "\n",
    "3. **Document Your Findings:**  \n",
    "   - Summarize the dataset and preprocessing steps.\n",
    "   - Describe the architectures used for both the FFNN and the CNN.\n",
    "   - Provide a comparative analysis discussing which model performed better and why, supported by TensorBoard screenshots or logged results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A - Choose your dataset Class\n",
    "\n",
    "Option 1 - As our lane line dataset from milestone 1 contains images, we are going to use the same dataset for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B - Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_height, resize_width = 256, 512\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "class TusimpleData(Dataset):\n",
    "    def __init__(self, dataset_file, n_labels=3, transform=None, target_transform=None, training=True, optuna=False):\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(dataset_file, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        self._shuffle()\n",
    "\n",
    "        purger = 1\n",
    "        if purger < 1.0 and training:\n",
    "            subset_size = int(len(self._gt_img_list) * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        zipped = list(zip(self._gt_img_list, self._gt_label_binary_list))\n",
    "        random.shuffle(zipped)\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*zipped)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._gt_img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self._gt_img_list[idx])\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "\n",
    "        label_binary = np.zeros((label_img.shape[0], label_img.shape[1]), dtype=np.uint8)\n",
    "        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        label_binary = torch.from_numpy(label_binary).long()\n",
    "        return img, label_binary\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C - FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3 * resize_height * resize_width\n",
    "output_dim = 2 * resize_height * resize_width\n",
    "\n",
    "class LaneLinesFNN(nn.Module):\n",
    "    def __init__(self, hidden1=1024, hidden2=256):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        logits = logits.view(-1, 2, resize_height, resize_width)\n",
    "        pred = torch.argmax(logits, dim=1, keepdim=True)\n",
    "        return {\"binary_seg_logits\": logits, \"binary_seg_pred\": pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSegLightningFNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesFNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fnn(model_ckpt_path):\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.makedirs('test_output')\n",
    "\n",
    "    img_path = '0001.png'\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LaneSegLightning.load_from_checkpoint(model_ckpt_path)\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    inp = Image.open(img_path)\n",
    "    input_tensor = transform(inp).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    binary_logits = output['binary_seg_logits']\n",
    "    binary_pred = output['binary_seg_pred']\n",
    "    binary_logits_np = binary_logits.detach().cpu().numpy()\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "\n",
    "    input_img = np.array(inp.resize((resize_width, resize_height)))\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "\n",
    "    cv2.imwrite(\"test_output/input.jpg\", input_img)\n",
    "    cv2.imwrite(\"test_output/binary_prediction.jpg\", binary_pred_np[0, 0] * 255)\n",
    "    cv2.imwrite(\"test_output/input_with_prediction_overlay.jpg\", overlay)\n",
    "\n",
    "    for i in range(binary_logits_np.shape[1]):\n",
    "        logits = binary_logits_np[0, i, :, :]\n",
    "        logits_norm = cv2.normalize(logits, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        cv2.imwrite(f\"test_output/binary_logits_channel_{i}.jpg\", logits_norm)\n",
    "\n",
    "    print(\"✅ Prediction visualization complete — see test_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneLinesCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 2, 3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        logits = self.deconv3(x)\n",
    "        pred = torch.argmax(logits, dim=1, keepdim=True)\n",
    "        return {\"binary_seg_logits\": logits, \"binary_seg_pred\": pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSegLightningCNN(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.model = LaneLinesCNN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, out, target):\n",
    "        logits = out[\"binary_seg_logits\"]\n",
    "        loss = self.loss_fn(logits, target) * 10\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        loss = self.compute_loss(out, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn(model_ckpt_path):\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.makedirs('test_output')\n",
    "\n",
    "    img_path = '0001.png'\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LaneSegLightning.load_from_checkpoint(model_ckpt_path)\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    inp = Image.open(img_path)\n",
    "    input_tensor = transform(inp).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    binary_logits = output['binary_seg_logits']\n",
    "    binary_pred = output['binary_seg_pred']\n",
    "    binary_logits_np = binary_logits.detach().cpu().numpy()\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "\n",
    "    input_img = np.array(inp.resize((resize_width, resize_height)))\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "\n",
    "    cv2.imwrite(\"test_output/input.jpg\", input_img)\n",
    "    cv2.imwrite(\"test_output/binary_prediction.jpg\", binary_pred_np[0, 0] * 255)\n",
    "    cv2.imwrite(\"test_output/input_with_prediction_overlay.jpg\", overlay)\n",
    "\n",
    "    for i in range(binary_logits_np.shape[1]):\n",
    "        logits = binary_logits_np[0, i, :, :]\n",
    "        logits_norm = cv2.normalize(logits, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        cv2.imwrite(f\"test_output/binary_logits_channel_{i}.jpg\", logits_norm)\n",
    "\n",
    "    print(\"✅ Prediction visualization complete — see test_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D - FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningFNN()\n",
    "logger = CSVLogger(\"logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_fnn(checkpoint.best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part D - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_ds = TusimpleData(train_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_ds = TusimpleData(val_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(train_ds)} samples for training.\")\n",
    "\n",
    "model = LaneSegLightningCNN()\n",
    "logger = CSVLogger(\"logs\", name=\"laneseg\")\n",
    "checkpoint = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"best_model\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[checkpoint], accelerator=\"auto\", devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "test_cnn(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Optimizers and Analyzing Training Curves\n",
    "\n",
    "In this step, you'll experiment with different optimizers—SGD, Adam, and RMSProp—to understand how they affect model performance. You will compare their effects using evaluation metrics on held-out test data and analyze the training and validation curves logged in TensorBoard.\n",
    "\n",
    "### A. Experiment Setup\n",
    "\n",
    "1. **Maintain Consistent Training Settings:**  \n",
    "   - Use the same model architecture (whether FFNN, CNN, or RNN from Parts 1 and 2) and dataset for all experiments.\n",
    "   - Ensure that the number of epochs, batch size, learning rate, and other hyperparameters are kept constant across different optimizer runs, aside from the optimizer itself.\n",
    "\n",
    "2. **Implement Optimizer Switching:**  \n",
    "   - Modify the `configure_optimizers` method in your PyTorch Lightning module to easily switch between optimizers:\n",
    "     ```python\n",
    "     def configure_optimizers(self):\n",
    "         # Uncomment the optimizer you want to use\n",
    "         # return torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "         # return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "         # return torch.optim.RMSprop(self.parameters(), lr=1e-3)\n",
    "     ```\n",
    "   - Train your model separately with each optimizer.\n",
    "\n",
    "### B. Evaluation Metrics and Analysis\n",
    "\n",
    "1. **Held-Out Test Evaluation:**  \n",
    "   - After training, evaluate each model on a held-out test set.\n",
    "   - Record quantitative metrics such as loss, accuracy, or any other relevant task-specific metric for each optimizer.\n",
    "\n",
    "2. **TensorBoard Analysis:**  \n",
    "   - Use TensorBoard to review the training and validation curves during training.\n",
    "   - Focus on:\n",
    "     - **Convergence Behavior:** How quickly does each optimizer reduce the loss?\n",
    "     - **Stability:** Are there noticeable fluctuations or instability in the curves?\n",
    "     - **Overfitting/Underfitting:** Do you observe signs of overfitting or underfitting, and how do these behaviors differ across optimizers?\n",
    "\n",
    "### C. Document Your Findings\n",
    "\n",
    "- **Summarize Performance:**  \n",
    "  - Create a table or a brief report comparing the evaluation metrics for SGD, Adam, and RMSProp.\n",
    "- **Include Visual Evidence:**  \n",
    "  - Attach TensorBoard screenshots or summaries of the logged training/validation curves.\n",
    "- **Provide a Comparative Analysis:**  \n",
    "  - Discuss which optimizer provided the best performance on the test set.\n",
    "  - Reflect on the convergence rates and stability differences you observed.\n",
    "  - Explain potential reasons for these differences based on your results.\n",
    "\n",
    "By the end of this exercise, you will have a deeper understanding of how different optimizers affect model training dynamics and performance. This insight is essential for making informed decisions when tuning models in future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "**What to Submit:**\n",
    "\n",
    "1. Your complete iPython notebook for Milestone 3 (including all code, outputs, and markdown explanations).\n",
    "2. A single PDF file that contains your entire report for the milestone, covering:\n",
    "   - Part 1: Benchmarking FFNN vs. RNN on sequence data.\n",
    "   - Part 2: (Any additional tasks, if applicable.)\n",
    "   - Part 3: Comparing optimizers and analyzing training curves.\n",
    "\n",
    "**How to Submit:**\n",
    "\n",
    "- Upload both your iPython notebook and the PDF report to Canvas.\n",
    "- Name your files clearly, for example:\n",
    "  - `YourName_Milestone3.ipynb`\n",
    "  - `YourName_Milestone3_Report.pdf`\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "- All submissions are due **4/18/21**.\n",
    "\n",
    "Happy Deep Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
