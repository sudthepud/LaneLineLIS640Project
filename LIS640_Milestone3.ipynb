{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3: Model Training and Evaluation with PyTorch Lightning\n",
    "\n",
    "Welcome to Milestone 3 of LIS 640 – Introduction to Applied Deep Learning. In this milestone, you'll build upon your work from Milestones 1 and 2 by upgrading your neural network baseline to a more robust training framework using PyTorch Lightning and TensorBoard logging. You will also be exploring the advantages of different neural architectures (recurrent and convolutional neural networks) and different optimizers.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The goal of Milestone 3 is to:\n",
    "- **Explore advanced architectures:** The main goal of Milestone 3 is to strengthen your knowledge about and experience with popular neural architectures including convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
    "- **Streamline your model development:** Make sure you are working with easy-to-maintain Lightning modules.\n",
    "- **Enhance experiment tracking:** Integrate TensorBoard to log and visualize training metrics, making it easier to monitor performance and debug issues.\n",
    "- **Investigate optimizer effects:** Experiment with different optimizers (such as Adam, SGD, and RMSprop) to understand their impact on model training and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Benchmarking Feedforward NN vs. RNN on Sequence Data\n",
    "\n",
    "In this step, you'll compare the performance of a Recurrent Neural Network (RNN) against a Feedforward Neural Network (FFNN) on a dataset that contains sequential data. **For this exercise, you must use PyTorch Lightning to build your models and manage the training loop, as well as TensorBoard for logging and visualizing your training metrics.**\n",
    "\n",
    "### A. Choose Your Dataset\n",
    "\n",
    "- **Option 1:**  \n",
    "  Use one of the datasets from Milestone 1 **if it contains sequence data**.  \n",
    "  *For example, if your dataset involves time series, text, or any ordered data, it qualifies for this comparison.* In that case you have already done part B and can skip on to part C.\n",
    "  \n",
    "\n",
    "- **Option 2:**  \n",
    "  If your Milestone 1 dataset does not include sequence data, search online for and download a dataset that features sequential information (e.g., time series forecasting, text classification, sensor data, etc.). Take inspiration from previous milestones on how to do part B (Data Preparation) for your new dataset.\n",
    "\n",
    "\n",
    "\n",
    "### B. Data Preparation\n",
    "\n",
    "1. **Create a Custom Dataset Class:**  \n",
    "   - Implement a PyTorch `Dataset` class that loads your sequence data.\n",
    "   - Include any necessary preprocessing steps (e.g., normalization, tokenization, padding for sequences).\n",
    "   - Ensure that your `__getitem__` method returns the data in a format suitable for your models.\n",
    "\n",
    "2. **Build DataLoaders:**  \n",
    "   - Use `torch.utils.data.DataLoader` to create train, validation, and test loaders.\n",
    "   - Choose appropriate batch sizes and shuffling to ensure effective training.\n",
    "\n",
    "### C. Model Implementation with PyTorch Lightning\n",
    "\n",
    "*Reuse implementations from Milestone 2 if that makes sense. The key difference now is that you should implement your models as PyTorch Lightning modules to take advantage of the built-in training loop and logging features.*\n",
    "\n",
    "1. **Feedforward Neural Network (FFNN):**  \n",
    "   - Implement a baseline feedforward network that treats the sequence data as independent features (e.g., by flattening the sequence).\n",
    "   - Keep the architecture simple to establish a baseline for comparison.\n",
    "\n",
    "2. **Recurrent Neural Network (RNN):**  \n",
    "   - Implement an RNN model (using LSTM or GRU) to handle the sequential nature of the data.\n",
    "   - Ensure that your model processes the sequence appropriately (e.g., using the final hidden state or an attention mechanism for prediction).\n",
    "\n",
    "*Remember to use the PyTorch Lightning `Trainer` for model training, and configure the module to log metrics to TensorBoard.*\n",
    "\n",
    "### D. Benchmarking and Evaluation\n",
    "\n",
    "1. **Training Both Models:**  \n",
    "   - Train both the FFNN and the RNN on your chosen dataset using similar training settings (e.g., number of epochs, learning rate, optimizer) to ensure a fair comparison.\n",
    "   - Use PyTorch Lightning’s `Trainer` to manage the training process.\n",
    "\n",
    "2. **Logging and Evaluation Metrics:**  \n",
    "   - Leverage TensorBoard logging to visualize training and validation metrics in real-time.\n",
    "   - Compare the performance of both models using metrics such as loss, accuracy, or any task-specific metric.\n",
    "   - Optionally, record additional statistics like training time or convergence behavior.\n",
    "\n",
    "3. **Document Your Findings:**  \n",
    "   - Summarize the dataset and preprocessing steps.\n",
    "   - Describe the architectures used for the FFNN and RNN.\n",
    "   - Provide a comparative analysis discussing which model performed better and why that might be the case.\n",
    "   - Include TensorBoard screenshots or logged results to support your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.75 GiB total capacity; 945.97 MiB already allocated; 91.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 240\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# ----- Training Loop -----\u001b[39;00m\n\u001b[1;32m    239\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 240\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLaneLinesRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m    242\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/modules/rnn.py:197\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m--> 197\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.75 GiB total capacity; 945.97 MiB already allocated; 91.81 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# ----- Settings -----\n",
    "resize_height, resize_width = 256, 512\n",
    "seq_len = 5  # Number of consecutive frames per sequence\n",
    "\n",
    "# ----- Data Transforms -----\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, tuple)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height)),\n",
    "])\n",
    "\n",
    "# ----- Dataset: Grouping Images into Sequences -----\n",
    "class SequenceTusimpleData(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset groups consecutive images (and their binary masks)\n",
    "    into sequences. The label is taken from the last frame in each sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, seq_len=5, n_labels=3, transform=None, target_transform=None, training=True, optuna=False):\n",
    "        self.seq_len = seq_len\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        \n",
    "        with open(dataset, 'r') as file:\n",
    "            for line in file:\n",
    "                info_tmp = line.strip().split()\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "                \n",
    "        # Sort the lists to (hopefully) preserve temporal order.\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*sorted(zip(self._gt_img_list, self._gt_label_binary_list)))\n",
    "        \n",
    "        # Optionally reduce dataset size for training.\n",
    "        purger = 0.2\n",
    "        if optuna:\n",
    "            purger = 0.01\n",
    "        if purger < 1.0 and training:\n",
    "            total_size = len(self._gt_img_list)\n",
    "            subset_size = int(total_size * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of sequences = total images - seq_len + 1\n",
    "        return len(self._gt_img_list) - self.seq_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Build a sequence of images from idx to idx+seq_len.\n",
    "        imgs = []\n",
    "        for i in range(self.seq_len):\n",
    "            img = Image.open(self._gt_img_list[idx + i])\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            imgs.append(img)\n",
    "        # Stack into a tensor with shape (seq_len, channels, H, W)\n",
    "        imgs = torch.stack(imgs, dim=0)\n",
    "        \n",
    "        # Use the binary mask from the last frame as the target.\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx + self.seq_len - 1], cv2.IMREAD_COLOR)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "        # Convert to binary mask (1 for non-black, 0 for black)\n",
    "        label_binary = np.zeros([label_img.shape[0], label_img.shape[1]], dtype=np.uint8)\n",
    "        mask = np.where((label_img != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        label_binary = torch.tensor(label_binary, dtype=torch.long)\n",
    "        \n",
    "        return imgs, label_binary\n",
    "\n",
    "# File paths for dataset text files.\n",
    "train_dataset_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_dataset_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "train_dataset = SequenceTusimpleData(train_dataset_file, seq_len=seq_len,\n",
    "                                     transform=data_transforms['train'],\n",
    "                                     target_transform=target_transforms, training=True)\n",
    "val_dataset = SequenceTusimpleData(val_dataset_file, seq_len=seq_len,\n",
    "                                   transform=data_transforms['val'],\n",
    "                                   target_transform=target_transforms, training=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "\n",
    "# ----- Model: RNN-based Lane Segmentation -----\n",
    "class LaneLinesRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=1024, seq_len=5):\n",
    "        super(LaneLinesRNN, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        # CNN encoder to extract features per frame.\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Pooling to reduce spatial dimensions.\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 16))  # Output: (batch, 128, 8, 16)\n",
    "        \n",
    "        # Flattened feature dimension.\n",
    "        self.feature_dim = 128 * 8 * 16  # 16384\n",
    "        \n",
    "        # LSTM to process sequence of features.\n",
    "        self.lstm = nn.LSTM(input_size=self.feature_dim, hidden_size=hidden_dim, num_layers=1, batch_first=False)\n",
    "        \n",
    "        # Map LSTM output back to CNN feature space.\n",
    "        self.fc = nn.Linear(hidden_dim, self.feature_dim)\n",
    "        \n",
    "        # Decoder: Upsample back to segmentation mask.\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x_seq):\n",
    "        # x_seq shape: (batch, seq_len, channels, H, W)\n",
    "        # Permute to (seq_len, batch, channels, H, W) for LSTM processing.\n",
    "        x_seq = x_seq.permute(1, 0, 2, 3, 4)\n",
    "        seq_len, batch_size, C, H, W = x_seq.size()\n",
    "        \n",
    "        encoded_features = []\n",
    "        # Process each frame through the CNN encoder.\n",
    "        for t in range(seq_len):\n",
    "            x = x_seq[t]  # (batch, C, H, W)\n",
    "            x = self.relu(self.conv1(x))\n",
    "            x = self.relu(self.conv2(x))\n",
    "            x = self.relu(self.conv3(x))\n",
    "            x = self.pool(x)  # (batch, 128, 8, 16)\n",
    "            x = x.view(batch_size, -1)  # Flatten to (batch, feature_dim)\n",
    "            encoded_features.append(x)\n",
    "        \n",
    "        features_seq = torch.stack(encoded_features, dim=0)  # (seq_len, batch, feature_dim)\n",
    "        \n",
    "        # Process the sequence with LSTM.\n",
    "        lstm_out, (h_n, c_n) = self.lstm(features_seq)\n",
    "        # Use the last hidden state.\n",
    "        last_hidden = h_n[0]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Map back to feature space.\n",
    "        fc_out = self.fc(last_hidden)  # (batch, feature_dim)\n",
    "        decoder_input = fc_out.view(batch_size, 128, 8, 16)\n",
    "        \n",
    "        # Decode to segmentation mask.\n",
    "        x = self.relu(self.deconv1(decoder_input))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        x = self.deconv3(x)  # (batch, 2, H, W)\n",
    "        binary_pred = torch.argmax(x, dim=1, keepdim=True)\n",
    "        return {\"binary_seg_logits\": x, \"binary_seg_pred\": binary_pred}\n",
    "\n",
    "# ----- Loss Function & Training/Validation Loops -----\n",
    "def compute_loss(net_output, binary_label):\n",
    "    k_binary = 10\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    binary_seg_logits = net_output[\"binary_seg_logits\"]\n",
    "    binary_loss = loss_fn(binary_seg_logits, binary_label)\n",
    "    binary_loss *= k_binary\n",
    "    total_loss = binary_loss\n",
    "    out = net_output[\"binary_seg_pred\"]\n",
    "    return total_loss, binary_loss, out\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    for inputs, binarys in dataloader:\n",
    "        # inputs: (batch, seq_len, channels, H, W)\n",
    "        inputs = inputs.float().to(device)\n",
    "        binarys = binarys.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        total_loss, binary_loss, _ = compute_loss(outputs, binarys)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += total_loss.item() * batch_size\n",
    "        running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "def test_loop(model, dataloader, device):\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, binarys in dataloader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            binarys = binarys.long().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, _ = compute_loss(outputs, binarys)\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += total_loss.item() * batch_size\n",
    "            running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "# ----- Training Loop -----\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneLinesRNN(hidden_dim=1024, seq_len=seq_len).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float(\"inf\")\n",
    "losses = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_loss_b = train_loop(model, dataloaders['train'], optimizer, scheduler, DEVICE)\n",
    "    print(f\"Training Loss: {train_loss:.4f} | Binary Loss: {train_loss_b:.4f}\")\n",
    "    val_loss, val_loss_b = test_loop(model, dataloaders['val'], DEVICE)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "    losses[epoch] = val_loss\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_wts, \"best_model.pth\")\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# ----- Testing Function -----\n",
    "def load_test_data(img_path, transform):\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "def test():\n",
    "    # Create output directory if needed.\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.mkdir('test_output')\n",
    "    \n",
    "    # For demonstration, we load a single image and duplicate it to form a sequence.\n",
    "    img_path = '0001.png'\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load and replicate the test image to create a sequence.\n",
    "    img = load_test_data(img_path, data_transform)\n",
    "    # Create a sequence: shape (seq_len, channels, H, W)\n",
    "    img_seq = torch.stack([img for _ in range(seq_len)], dim=0)\n",
    "    # Add batch dimension: (1, seq_len, channels, H, W)\n",
    "    img_seq = torch.unsqueeze(img_seq, dim=0)\n",
    "    \n",
    "    # Load best model.\n",
    "    model = LaneLinesRNN(hidden_dim=1024, seq_len=seq_len)\n",
    "    state_dict = torch.load(\"best_model.pth\", map_location=DEVICE)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_seq.to(DEVICE))\n",
    "    \n",
    "    # Process output: overlay prediction on the original image.\n",
    "    input_img = Image.open(img_path).resize((resize_width, resize_height))\n",
    "    input_img_np = np.array(input_img)\n",
    "    binary_pred = outputs['binary_seg_pred']\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "    \n",
    "    overlay = input_img_np.copy()\n",
    "    # Overlay in red where prediction is positive.\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "    cv2.imwrite(os.path.join('test_output', 'input_with_prediction_overlay.jpg'), overlay)\n",
    "    print(\"Test output saved to 'test_output/input_with_prediction_overlay.jpg'.\")\n",
    "\n",
    "# ----- Run Testing -----\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Benchmarking Feedforward NN vs. CNN on Image Data\n",
    "\n",
    "In this step, you'll compare the performance of a Convolutional Neural Network (CNN) against a Feedforward Neural Network (FFNN) on an image-based dataset. **For this exercise, you must use PyTorch Lightning to implement your models and manage training, and use TensorBoard for logging and visualizing your training metrics.**\n",
    "\n",
    "### A. Choose Your Dataset\n",
    "\n",
    "- **Option 1:**  \n",
    "  Use one of the datasets from Milestone 1 **if it contains image data**.  \n",
    "  *For example, if your dataset involves images for classification, segmentation, or any visual task, it qualifies for this comparison.*\n",
    "\n",
    "- **Option 2:**  \n",
    "  If your Milestone 1 dataset does not include image data, search online for and download an image dataset (e.g., Fashion MNIST, CIFAR-10, or any domain-specific image dataset).\n",
    "\n",
    "### B. Data Preparation\n",
    "\n",
    "1. **Create a Custom Dataset Class:**  \n",
    "   - Implement a PyTorch `Dataset` class that loads your image data.\n",
    "   - Include any necessary preprocessing steps (e.g., normalization, resizing, data augmentation).\n",
    "   - Ensure that your `__getitem__` method returns the data in a format suitable for your models.\n",
    "\n",
    "2. **Build DataLoaders:**  \n",
    "   - Use `torch.utils.data.DataLoader` to create train, validation, and test loaders.\n",
    "   - Choose appropriate batch sizes and apply shuffling to ensure effective training.\n",
    "\n",
    "### C. Model Implementation with PyTorch Lightning\n",
    "\n",
    "*Reuse or adapt implementations from Milestone 2 as needed. The key requirement is to implement your models as PyTorch Lightning modules to take advantage of the built-in training loop and logging features.*\n",
    "\n",
    "1. **Feedforward Neural Network (FFNN):**  \n",
    "   - Implement a baseline FFNN that treats image data as a flat vector (i.e., by flattening the image).\n",
    "   - Keep the architecture simple to serve as a baseline for comparison.\n",
    "\n",
    "2. **Convolutional Neural Network (CNN):**  \n",
    "   - Implement a CNN architecture that leverages convolutional layers to capture spatial hierarchies in the image data.\n",
    "   - Typical layers might include convolution, activation (ReLU), pooling, and fully connected layers.\n",
    "   - Ensure that your model architecture is designed to process image data effectively.\n",
    "\n",
    "*Remember to use the PyTorch Lightning `Trainer` for training and to configure your Lightning module to log metrics to TensorBoard.*\n",
    "\n",
    "### D. Benchmarking and Evaluation\n",
    "\n",
    "1. **Training Both Models:**  \n",
    "   - Train both the FFNN and the CNN on your chosen dataset using similar training settings (e.g., number of epochs, learning rate, optimizer) to ensure a fair comparison.\n",
    "   - Use PyTorch Lightning’s `Trainer` to manage the training process.\n",
    "\n",
    "2. **Logging and Evaluation Metrics:**  \n",
    "   - Leverage TensorBoard to log and visualize training and validation metrics in real-time.\n",
    "   - Compare the performance of both models using metrics such as loss, accuracy, or any task-specific evaluation metric.\n",
    "   - Optionally, record additional details like training time and convergence behavior.\n",
    "\n",
    "3. **Document Your Findings:**  \n",
    "   - Summarize the dataset and preprocessing steps.\n",
    "   - Describe the architectures used for both the FFNN and the CNN.\n",
    "   - Provide a comparative analysis discussing which model performed better and why, supported by TensorBoard screenshots or logged results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Optimizers and Analyzing Training Curves\n",
    "\n",
    "In this step, you'll experiment with different optimizers—SGD, Adam, and RMSProp—to understand how they affect model performance. You will compare their effects using evaluation metrics on held-out test data and analyze the training and validation curves logged in TensorBoard.\n",
    "\n",
    "### A. Experiment Setup\n",
    "\n",
    "1. **Maintain Consistent Training Settings:**  \n",
    "   - Use the same model architecture (whether FFNN, CNN, or RNN from Parts 1 and 2) and dataset for all experiments.\n",
    "   - Ensure that the number of epochs, batch size, learning rate, and other hyperparameters are kept constant across different optimizer runs, aside from the optimizer itself.\n",
    "\n",
    "2. **Implement Optimizer Switching:**  \n",
    "   - Modify the `configure_optimizers` method in your PyTorch Lightning module to easily switch between optimizers:\n",
    "     ```python\n",
    "     def configure_optimizers(self):\n",
    "         # Uncomment the optimizer you want to use\n",
    "         # return torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "         # return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "         # return torch.optim.RMSprop(self.parameters(), lr=1e-3)\n",
    "     ```\n",
    "   - Train your model separately with each optimizer.\n",
    "\n",
    "### B. Evaluation Metrics and Analysis\n",
    "\n",
    "1. **Held-Out Test Evaluation:**  \n",
    "   - After training, evaluate each model on a held-out test set.\n",
    "   - Record quantitative metrics such as loss, accuracy, or any other relevant task-specific metric for each optimizer.\n",
    "\n",
    "2. **TensorBoard Analysis:**  \n",
    "   - Use TensorBoard to review the training and validation curves during training.\n",
    "   - Focus on:\n",
    "     - **Convergence Behavior:** How quickly does each optimizer reduce the loss?\n",
    "     - **Stability:** Are there noticeable fluctuations or instability in the curves?\n",
    "     - **Overfitting/Underfitting:** Do you observe signs of overfitting or underfitting, and how do these behaviors differ across optimizers?\n",
    "\n",
    "### C. Document Your Findings\n",
    "\n",
    "- **Summarize Performance:**  \n",
    "  - Create a table or a brief report comparing the evaluation metrics for SGD, Adam, and RMSProp.\n",
    "- **Include Visual Evidence:**  \n",
    "  - Attach TensorBoard screenshots or summaries of the logged training/validation curves.\n",
    "- **Provide a Comparative Analysis:**  \n",
    "  - Discuss which optimizer provided the best performance on the test set.\n",
    "  - Reflect on the convergence rates and stability differences you observed.\n",
    "  - Explain potential reasons for these differences based on your results.\n",
    "\n",
    "By the end of this exercise, you will have a deeper understanding of how different optimizers affect model training dynamics and performance. This insight is essential for making informed decisions when tuning models in future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "**What to Submit:**\n",
    "\n",
    "1. Your complete iPython notebook for Milestone 3 (including all code, outputs, and markdown explanations).\n",
    "2. A single PDF file that contains your entire report for the milestone, covering:\n",
    "   - Part 1: Benchmarking FFNN vs. RNN on sequence data.\n",
    "   - Part 2: (Any additional tasks, if applicable.)\n",
    "   - Part 3: Comparing optimizers and analyzing training curves.\n",
    "\n",
    "**How to Submit:**\n",
    "\n",
    "- Upload both your iPython notebook and the PDF report to Canvas.\n",
    "- Name your files clearly, for example:\n",
    "  - `YourName_Milestone3.ipynb`\n",
    "  - `YourName_Milestone3_Report.pdf`\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "- All submissions are due **4/18/21**.\n",
    "\n",
    "Happy Deep Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
