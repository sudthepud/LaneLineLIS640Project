{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 2: Neural Network Baseline and Hyperparameter Optimization\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 3/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In Milestone 1 you have:\n",
    "1. **Defined a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identified three datasets** that fit your topic and justified their relevance.\n",
    "3. **Explored and visualized** the datasets to understand their structure.\n",
    "4. **Implemented a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "In Milestone 2 we will take the next step and implement a neural network baseline based on what we have learned in class! For this milestone, please use one of the datasets you picked in the last milestone. If you pick a new one, make sure to do Steps 2 - 4 again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "\n",
    "The first step is to be clear about what you want your model to predict. Is your goal a classification or a regression task? what are the input features and what are you prediction targets y? Make sure that you have a sensible choice of features and a sensible choice of prediction targets y in your dataloader.\n",
    "\n",
    "**Write down one paragraph of justification for how you set up your DataLoader below. If it makes sense to change the DataLoader from Milestone 1, describe what you changed and why:** \n",
    "We decided to completely switch our dataset to the TuSimple lane lines dataset. The reason for this is because we found that our roboflow datasets we were using before were not uniform at all in the labelling, while the TuSimple dataset provided 10000+ images with consistent labelling. So our DataLoader consists of the TuSimple dataset, where the labels are a binary mask (white or black) over an image indicating where the lane lines are. We decrease the size of each image heavily down to 512x256 to make the dataset more manageable and also implemented a amount of dataset used percentage to control how much of the 10000 images we actually use since training on all 10000 images is very time consuming. To get the TuSimple dataset formatted and set up into a DataLoader, we took inspiration from this project we found (https://github.com/IrohXu/lanenet-lane-detection-pytorch) and learned how they modified the TuSimple dataset and what transformations they applied to make the data better for training. They also implemented data shuffling so that each epoch sees the data in a different order, which we thought would be a good idea so we implemented that into our DataLoader as well. We then created a dictionary with our train and test DataLoader for ease of accessing while training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Train a Neural Network in PyTorch**\n",
    "\n",
    "We learned in class how to implement and train a feed forward neural network in pytorch. You can find reference implementations [here](https://github.com/mariru/Intro2ADL/blob/main/Week5/Week5_Lab_Example.ipynb) and [here](https://www.kaggle.com/code/girlboss/mmlm2025-pytorch-lb-0-00000). Tip: Try to implement the neural network by yourself from scratch before looking at the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d971bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "# define dataloaders: make sure to have a train, validation and a test loader\n",
    "train_dataset_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_dataset_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "resize_height, resize_width = 256, 512\n",
    "\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample = cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "        return sample\n",
    "\n",
    "class TusimpleData(Dataset):\n",
    "    def __init__(self, dataset, n_labels=3, transform=None, target_transform=None, training=True):\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(dataset, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        self._shuffle()\n",
    "\n",
    "        # DECREASE AMOUNT OF DATA\n",
    "        purger = 0.01\n",
    "        if purger < 1.0 and training:\n",
    "            total_size = len(self._gt_img_list)\n",
    "            subset_size = int(total_size * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        c = list(zip(self._gt_img_list, self._gt_label_binary_list))\n",
    "        random.shuffle(c)\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*c)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._gt_img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self._gt_img_list[idx])\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "        label_binary = np.zeros([label_img.shape[0], label_img.shape[1]], dtype=np.uint8)\n",
    "        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        return img, label_binary\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height)),\n",
    "])\n",
    "\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(val_loader.dataset)\n",
    "}\n",
    "\n",
    "# define the model\n",
    "class LaneLines(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaneLines, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.deconv1_binary = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2_binary = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3_binary = nn.ConvTranspose2d(32, 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        binary = self.relu(self.deconv1_binary(x))\n",
    "        binary = self.relu(self.deconv2_binary(binary))\n",
    "        binary = self.deconv3_binary(binary)\n",
    "        binary_pred = torch.argmax(binary, dim=1, keepdim=True)\n",
    "        return {\n",
    "            \"binary_seg_logits\": binary,\n",
    "            \"binary_seg_pred\": binary_pred\n",
    "        }\n",
    "\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "def compute_loss(net_output, binary_label):\n",
    "    k_binary = 10\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    binary_seg_logits = net_output[\"binary_seg_logits\"]\n",
    "    binary_loss = loss_fn(binary_seg_logits, binary_label)\n",
    "    binary_loss = binary_loss * k_binary\n",
    "    total_loss = binary_loss\n",
    "    out = net_output[\"binary_seg_pred\"]\n",
    "    return total_loss, binary_loss, out\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    for inputs, binarys in dataloader:\n",
    "        inputs = inputs.float().to(device)\n",
    "        binarys = binarys.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True): \n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, out = compute_loss(outputs, binarys)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += total_loss.item() * batch_size\n",
    "        running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "def test_loop(model, dataloader, device):\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, binarys in dataloader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            binarys = binarys.long().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, out = compute_loss(outputs, binarys)\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += total_loss.item() * batch_size\n",
    "            running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "# # train the model\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = LaneLines().to(DEVICE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# num_epochs = 100\n",
    "# best_model_wts = copy.deepcopy(model.state_dict())\n",
    "# best_loss = float(\"inf\")\n",
    "# losses = {}\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, DEVICE)\n",
    "#     print(f\"Training Loss: {train_loss} | Binary Loss: {train_loss_b}\")\n",
    "#     val_loss, val_loss_b = test_loop(model, val_dataloader, DEVICE)\n",
    "#     print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "#     losses[epoch] = val_loss\n",
    "\n",
    "#     if val_loss < best_loss:\n",
    "#         best_loss = val_loss\n",
    "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#         torch.save(best_model_wts, \"best_model.pth\")\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "\n",
    "# # test the model\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def load_test_data(img_path, transform):\n",
    "#     img = Image.open(img_path)\n",
    "#     img = transform(img)\n",
    "#     return img\n",
    "\n",
    "# def test():\n",
    "#     if not os.path.exists('test_output'):\n",
    "#         os.mkdir('test_output')\n",
    "#     img_path = '0001.png'\n",
    "#     resize_height, resize_width = 256, 512\n",
    "#     model_path = 'best_model.pth'\n",
    "#     data_transform = transforms.Compose([\n",
    "#         transforms.Resize((resize_height, resize_width)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "    \n",
    "#     model = LaneLines()\n",
    "#     state_dict = torch.load(model_path)\n",
    "#     model.load_state_dict(state_dict)\n",
    "#     model.eval()\n",
    "#     model.to(DEVICE)\n",
    "#     inp = load_test_data(img_path, data_transform).to(DEVICE)\n",
    "#     inp = torch.unsqueeze(inp, dim=0)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(inp)\n",
    "#     input_img = Image.open(img_path)\n",
    "#     input_img = input_img.resize((resize_width, resize_height))\n",
    "#     input_img = np.array(input_img)\n",
    "#     binary_pred = outputs['binary_seg_pred']\n",
    "#     binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "#     overlay = input_img.copy()\n",
    "#     overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "#     cv2.imwrite(os.path.join('test_output', 'input_with_prediction_overlay.jpg'), overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 2 continued: Try Stuff**\n",
    "\n",
    "Use your code above to try different architectures. Make sure to use early stopping! Try adding Dropout and BatchNorm, try different learning rates. How do they affect training and validation performance? \n",
    "\n",
    " **Summarize your observations in a paragraph below:**\n",
    " Since we have a large dataset, we decided to try this with only 10% of the data (1000 images). We found that deeper networks made training way more time consuming without giving us much benefit in performance of the final model, and we also found that Dropout did help with overfitting and slightly improved inference of our model, but only when we trained the model on a larger portion of the dataset, so we added Dropout. We also found that BatchNorm did not make a big difference. We found that the best learning rate was in the range of 0.001 to 0.004. We also implemented early stopping by keeping track of each epoch's loss on validation and stopping if the loss is worse than the previous 5 epochs. We hope to use the extension to mess around with different things more, since our biggest limitation has been time to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**\n",
    "\n",
    "As you can see, hyperparameter optimization can be tedious. In class we used [optuna](https://optuna.org/#code_examples) to automate the process. Your next task is to wrap your code from Step 2 into an objective which you can then optimize with optuna. Under the [code exaples](https://optuna.org/#code_examples) there is a tab *PyTorch* which should be helpful as it provides a minimal example on how to wrap PyTorch code inside an objective.\n",
    "\n",
    "**Important: Make sure the model is evaluated on a validation set, not the training data!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d1593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 21:33:05,178] A new study created in memory with name: no-name-771290f2-e20f-46ac-8cb6-792c91c130d7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_345008/1820681752.py:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)  # Learning rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 759.7092 | Binary Loss: 759.7092\n",
      "Validation Loss: 6274.7730 | Binary Loss: 6274.7730\n",
      "Epoch 2/10\n",
      "Training Loss: 743.5794 | Binary Loss: 743.5794\n",
      "Validation Loss: 6062.7221 | Binary Loss: 6062.7221\n",
      "Epoch 3/10\n",
      "Training Loss: 715.9781 | Binary Loss: 715.9781\n",
      "Validation Loss: 5794.1059 | Binary Loss: 5794.1059\n",
      "Epoch 4/10\n",
      "Training Loss: 683.0329 | Binary Loss: 683.0329\n",
      "Validation Loss: 5505.1359 | Binary Loss: 5505.1359\n",
      "Epoch 5/10\n",
      "Training Loss: 648.2763 | Binary Loss: 648.2763\n",
      "Validation Loss: 5213.3020 | Binary Loss: 5213.3020\n",
      "Epoch 6/10\n",
      "Training Loss: 621.9341 | Binary Loss: 621.9341\n",
      "Validation Loss: 5184.1991 | Binary Loss: 5184.1991\n",
      "Epoch 7/10\n",
      "Training Loss: 618.4460 | Binary Loss: 618.4460\n",
      "Validation Loss: 5154.8244 | Binary Loss: 5154.8244\n",
      "Epoch 8/10\n",
      "Training Loss: 614.9332 | Binary Loss: 614.9332\n",
      "Validation Loss: 5125.3781 | Binary Loss: 5125.3781\n",
      "Epoch 9/10\n",
      "Training Loss: 611.4105 | Binary Loss: 611.4105\n",
      "Validation Loss: 5095.9629 | Binary Loss: 5095.9629\n",
      "Epoch 10/10\n",
      "Training Loss: 607.8986 | Binary Loss: 607.8986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 21:36:42,436] Trial 0 finished with value: 5066.6619601249695 and parameters: {'lr': 0.000203755613237648, 'batch_size': 32, 'optimizer': 'SGD'}. Best is trial 0 with value: 5066.6619601249695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5066.6620 | Binary Loss: 5066.6620\n",
      "Epoch 1/10\n",
      "Training Loss: 558.6253 | Binary Loss: 558.6253\n",
      "Validation Loss: 1314.4107 | Binary Loss: 1314.4107\n",
      "Epoch 2/10\n",
      "Training Loss: 137.9629 | Binary Loss: 137.9629\n",
      "Validation Loss: 881.9060 | Binary Loss: 881.9060\n",
      "Epoch 3/10\n",
      "Training Loss: 102.5860 | Binary Loss: 102.5860\n",
      "Validation Loss: 804.1178 | Binary Loss: 804.1178\n",
      "Epoch 4/10\n",
      "Training Loss: 95.2113 | Binary Loss: 95.2113\n",
      "Validation Loss: 746.5355 | Binary Loss: 746.5355\n",
      "Epoch 5/10\n",
      "Training Loss: 90.4966 | Binary Loss: 90.4966\n",
      "Validation Loss: 761.0991 | Binary Loss: 761.0991\n",
      "Epoch 6/10\n",
      "Training Loss: 90.5807 | Binary Loss: 90.5807\n",
      "Validation Loss: 714.1899 | Binary Loss: 714.1899\n",
      "Epoch 7/10\n",
      "Training Loss: 86.5790 | Binary Loss: 86.5790\n",
      "Validation Loss: 731.2047 | Binary Loss: 731.2047\n",
      "Epoch 8/10\n",
      "Training Loss: 87.0029 | Binary Loss: 87.0029\n",
      "Validation Loss: 705.4856 | Binary Loss: 705.4856\n",
      "Epoch 9/10\n",
      "Training Loss: 85.3907 | Binary Loss: 85.3907\n",
      "Validation Loss: 708.0845 | Binary Loss: 708.0845\n",
      "Epoch 10/10\n",
      "Training Loss: 84.9166 | Binary Loss: 84.9166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 21:40:20,079] Trial 1 finished with value: 705.4856085777283 and parameters: {'lr': 0.006664870397791646, 'batch_size': 32, 'optimizer': 'Adam'}. Best is trial 1 with value: 705.4856085777283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 705.5539 | Binary Loss: 705.5539\n",
      "Epoch 1/10\n",
      "Training Loss: 675.1977 | Binary Loss: 675.1977\n",
      "Validation Loss: 5631.7895 | Binary Loss: 5631.7895\n",
      "Epoch 2/10\n",
      "Training Loss: 671.4233 | Binary Loss: 671.4233\n",
      "Validation Loss: 5581.2349 | Binary Loss: 5581.2349\n",
      "Epoch 3/10\n",
      "Training Loss: 664.7571 | Binary Loss: 664.7571\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna.exceptions\n",
    "\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "\n",
    "# Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LaneLines().to(device)\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    # loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"Training Loss: {train_loss:.4f} | Binary Loss: {train_loss_b:.4f}\")\n",
    "\n",
    "        val_loss, val_loss_b = test_loop(model, val_dataloader, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "\n",
    "# Optimize the objective function\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print out the best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b007",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**\n",
    "\n",
    "The hyperparameter search was somewhat helpful, but we think we werenâ€™t able to harness a lot of the potential benefits due to the significant training times when running the Optuna trials â€“ even with only using a small percentage (5-10%) of the data. While increasing the number of trials did yield hyperparameters that improved results, we could not increase the number of trials by a significant amount. Additionally, we found the pruning feature on the Optuna website and found it to be useful in cutting off less promising trials early, which allowed us to bump up the number of trials. We plan to look into potentially using cloud compute so we can run more, longer trials for the hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54312a3",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n",
    "\n",
    "Now that you have found a good hyperparameter setting the validation set is no longer needed. The last step is to combine the training and validation set into a combined training set and retrain the model under the best parameter setting found. Report your final loss on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_lr = study.best_params['lr']\n",
    "optuna_batch_size = study.best_params['batch_size']\n",
    "optuna_optimizer = study.best_params['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     train_loss, train_loss_b \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Binary Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     val_loss, val_loss_b \u001b[38;5;241m=\u001b[39m test_loop(model, val_dataloader, DEVICE)\n",
      "Cell \u001b[0;32mIn[32], line 162\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m    159\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    160\u001b[0m running_loss_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, binarys \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    163\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    164\u001b[0m     binarys \u001b[38;5;241m=\u001b[39m binarys\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[32], line 79\u001b[0m, in \u001b[0;36mTusimpleData.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     77\u001b[0m label_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gt_label_binary_list[idx], cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 79\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[1;32m     81\u001b[0m     label_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label_img)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1285\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_contrast(img, contrast_factor)\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1285\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_saturation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaturation_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1287\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/transforms/functional.py:937\u001b[0m, in \u001b[0;36madjust_saturation\u001b[0;34m(img, saturation_factor)\u001b[0m\n\u001b[1;32m    935\u001b[0m     _log_api_usage_once(adjust_saturation)\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_saturation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaturation_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:93\u001b[0m, in \u001b[0;36madjust_saturation\u001b[0;34m(img, saturation_factor)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m enhancer \u001b[38;5;241m=\u001b[39m ImageEnhance\u001b[38;5;241m.\u001b[39mColor(img)\n\u001b[0;32m---> 93\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43menhancer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menhance\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaturation_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/PIL/ImageEnhance.py:40\u001b[0m, in \u001b[0;36m_Enhance.enhance\u001b[0;34m(self, factor)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21menhance\u001b[39m(\u001b[38;5;28mself\u001b[39m, factor: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Returns an enhanced image.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    :rtype: :py:class:`~PIL.Image.Image`\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegenerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov10/lib/python3.9/site-packages/PIL/Image.py:3540\u001b[0m, in \u001b[0;36mblend\u001b[0;34m(im1, im2, alpha)\u001b[0m\n\u001b[1;32m   3538\u001b[0m im1\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   3539\u001b[0m im2\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m-> 3540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im1\u001b[38;5;241m.\u001b[39m_new(\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=False)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneLines().to(DEVICE)\n",
    "\n",
    "if optuna_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=optuna_lr, momentum=0.9)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=optuna_lr)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=optuna_batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=optuna_batch_size, shuffle=False)\n",
    "num_epochs = 100\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float(\"inf\")\n",
    "losses = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, DEVICE)\n",
    "    print(f\"Training Loss: {train_loss} | Binary Loss: {train_loss_b}\")\n",
    "    val_loss, val_loss_b = test_loop(model, val_dataloader, DEVICE)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "    losses[epoch] = val_loss\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_wts, \"best_model.pth\")\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# test the model\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_test_data(img_path, transform):\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "def test():\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.mkdir('test_output')\n",
    "    img_path = '0001.png'\n",
    "    resize_height, resize_width = 256, 512\n",
    "    model_path = 'best_model.pth'\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    model = LaneLines()\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    inp = load_test_data(img_path, data_transform).to(DEVICE)\n",
    "    inp = torch.unsqueeze(inp, dim=0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inp)\n",
    "    input_img = Image.open(img_path)\n",
    "    input_img = input_img.resize((resize_width, resize_height))\n",
    "    input_img = np.array(input_img)\n",
    "    binary_pred = outputs['binary_seg_pred']\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "    cv2.imwrite(os.path.join('test_output', 'input_with_prediction_overlay.jpg'), overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 2 to Canvas. \n",
    "Happy Deep Learning! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
