{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 2: Neural Network Baseline and Hyperparameter Optimization\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 3/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In Milestone 1 you have:\n",
    "1. **Defined a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identified three datasets** that fit your topic and justified their relevance.\n",
    "3. **Explored and visualized** the datasets to understand their structure.\n",
    "4. **Implemented a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "In Milestone 2 we will take the next step and implement a neural network baseline based on what we have learned in class! For this milestone, please use one of the datasets you picked in the last milestone. If you pick a new one, make sure to do Steps 2 - 4 again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "\n",
    "The first step is to be clear about what you want your model to predict. Is your goal a classification or a regression task? what are the input features and what are you prediction targets y? Make sure that you have a sensible choice of features and a sensible choice of prediction targets y in your dataloader.\n",
    "\n",
    "**Write down one paragraph of justification for how you set up your DataLoader below. If it makes sense to change the DataLoader from Milestone 1, describe what you changed and why:** \n",
    "We decided to completely switch our dataset to the TuSimple lane lines dataset. The reason for this is because we found that our roboflow datasets we were using before were not uniform at all in the labelling, while the TuSimple dataset provided 10000+ images with consistent labelling. So our DataLoader consists of the TuSimple dataset, where the labels are a binary mask (white or black) over an image indicating where the lane lines are. We decrease the size of each image heavily down to 512x256 to make the dataset more manageable and also implemented a amount of dataset used percentage to control how much of the 10000 images we actually use since training on all 10000 images is very time consuming. To get the TuSimple dataset formatted and set up into a DataLoader, we took inspiration from this project we found (https://github.com/IrohXu/lanenet-lane-detection-pytorch) and learned how they modified the TuSimple dataset and what transformations they applied to make the data better for training. They also implemented data shuffling so that each epoch sees the data in a different order, which we thought would be a good idea so we implemented that into our DataLoader as well. We then created a dictionary with our train and test DataLoader for ease of accessing while training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Train a Neural Network in PyTorch**\n",
    "\n",
    "We learned in class how to implement and train a feed forward neural network in pytorch. You can find reference implementations [here](https://github.com/mariru/Intro2ADL/blob/main/Week5/Week5_Lab_Example.ipynb) and [here](https://www.kaggle.com/code/girlboss/mmlm2025-pytorch-lb-0-00000). Tip: Try to implement the neural network by yourself from scratch before looking at the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d971bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "# define dataloaders: make sure to have a train, validation and a test loader\n",
    "train_dataset_file = 'archive/TUSimple/train_set/training/train.txt'\n",
    "val_dataset_file = 'archive/TUSimple/train_set/training/val.txt'\n",
    "\n",
    "resize_height, resize_width = 256, 512\n",
    "\n",
    "class Rescale():\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample = cv2.resize(sample, dsize=self.output_size, interpolation=cv2.INTER_NEAREST)\n",
    "        return sample\n",
    "\n",
    "class TusimpleData(Dataset):\n",
    "    def __init__(self, dataset, n_labels=3, transform=None, target_transform=None, training=True, optuna=False):\n",
    "        self._gt_img_list = []\n",
    "        self._gt_label_binary_list = []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        with open(dataset, 'r') as file:\n",
    "            for _info in file:\n",
    "                info_tmp = _info.strip(' ').split()\n",
    "\n",
    "                self._gt_img_list.append(info_tmp[0])\n",
    "                self._gt_label_binary_list.append(info_tmp[1])\n",
    "\n",
    "        self._shuffle()\n",
    "\n",
    "        # DECREASE AMOUNT OF DATA\n",
    "        purger = 0.2\n",
    "        if optuna:\n",
    "            purger = 0.01\n",
    "        if purger < 1.0 and training:\n",
    "            total_size = len(self._gt_img_list)\n",
    "            subset_size = int(total_size * purger)\n",
    "            self._gt_img_list = self._gt_img_list[:subset_size]\n",
    "            self._gt_label_binary_list = self._gt_label_binary_list[:subset_size]\n",
    "\n",
    "    def _shuffle(self):\n",
    "        c = list(zip(self._gt_img_list, self._gt_label_binary_list))\n",
    "        random.shuffle(c)\n",
    "        self._gt_img_list, self._gt_label_binary_list = zip(*c)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._gt_img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self._gt_img_list[idx])\n",
    "        label_img = cv2.imread(self._gt_label_binary_list[idx], cv2.IMREAD_COLOR)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label_img = self.target_transform(label_img)\n",
    "        label_binary = np.zeros([label_img.shape[0], label_img.shape[1]], dtype=np.uint8)\n",
    "        mask = np.where((label_img[:, :, :] != [0, 0, 0]).all(axis=2))\n",
    "        label_binary[mask] = 1\n",
    "        return img, label_binary\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "target_transforms = transforms.Compose([\n",
    "    Rescale((resize_width, resize_height)),\n",
    "])\n",
    "\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset),\n",
    "    'val': len(val_loader.dataset)\n",
    "}\n",
    "\n",
    "# define the model\n",
    "class LaneLines(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaneLines, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.deconv1_binary = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2_binary = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3_binary = nn.ConvTranspose2d(32, 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        binary = self.relu(self.deconv1_binary(x))\n",
    "        binary = self.relu(self.deconv2_binary(binary))\n",
    "        binary = self.deconv3_binary(binary)\n",
    "        binary_pred = torch.argmax(binary, dim=1, keepdim=True)\n",
    "        return {\n",
    "            \"binary_seg_logits\": binary,\n",
    "            \"binary_seg_pred\": binary_pred\n",
    "        }\n",
    "\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "def compute_loss(net_output, binary_label):\n",
    "    k_binary = 10\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    binary_seg_logits = net_output[\"binary_seg_logits\"]\n",
    "    binary_loss = loss_fn(binary_seg_logits, binary_label)\n",
    "    binary_loss = binary_loss * k_binary\n",
    "    total_loss = binary_loss\n",
    "    out = net_output[\"binary_seg_pred\"]\n",
    "    return total_loss, binary_loss, out\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    for inputs, binarys in dataloader:\n",
    "        inputs = inputs.float().to(device)\n",
    "        binarys = binarys.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True): \n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, out = compute_loss(outputs, binarys)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += total_loss.item() * batch_size\n",
    "        running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "def test_loop(model, dataloader, device):\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    running_loss_b = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, binarys in dataloader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            binarys = binarys.long().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            total_loss, binary_loss, out = compute_loss(outputs, binarys)\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += total_loss.item() * batch_size\n",
    "            running_loss_b += binary_loss.item() * batch_size\n",
    "\n",
    "    return running_loss, running_loss_b\n",
    "\n",
    "\n",
    "# Commnted out training loop as it takes too long to run and repeating this in the last step\n",
    "\n",
    "# # train the model\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = LaneLines().to(DEVICE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# num_epochs = 100\n",
    "# best_model_wts = copy.deepcopy(model.state_dict())\n",
    "# best_loss = float(\"inf\")\n",
    "# losses = {}\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, DEVICE)\n",
    "#     print(f\"Training Loss: {train_loss} | Binary Loss: {train_loss_b}\")\n",
    "#     val_loss, val_loss_b = test_loop(model, val_dataloader, DEVICE)\n",
    "#     print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "#     losses[epoch] = val_loss\n",
    "\n",
    "#     if val_loss < best_loss:\n",
    "#         best_loss = val_loss\n",
    "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#         torch.save(best_model_wts, \"best_model.pth\")\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "\n",
    "# # test the model\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def load_test_data(img_path, transform):\n",
    "#     img = Image.open(img_path)\n",
    "#     img = transform(img)\n",
    "#     return img\n",
    "\n",
    "# def test():\n",
    "#     if not os.path.exists('test_output'):\n",
    "#         os.mkdir('test_output')\n",
    "#     img_path = '0001.png'\n",
    "#     resize_height, resize_width = 256, 512\n",
    "#     model_path = 'best_model.pth'\n",
    "#     data_transform = transforms.Compose([\n",
    "#         transforms.Resize((resize_height, resize_width)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "    \n",
    "#     model = LaneLines()\n",
    "#     state_dict = torch.load(model_path)\n",
    "#     model.load_state_dict(state_dict)\n",
    "#     model.eval()\n",
    "#     model.to(DEVICE)\n",
    "#     inp = load_test_data(img_path, data_transform).to(DEVICE)\n",
    "#     inp = torch.unsqueeze(inp, dim=0)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(inp)\n",
    "#     input_img = Image.open(img_path)\n",
    "#     input_img = input_img.resize((resize_width, resize_height))\n",
    "#     input_img = np.array(input_img)\n",
    "#     binary_pred = outputs['binary_seg_pred']\n",
    "#     binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "#     overlay = input_img.copy()\n",
    "#     overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "#     cv2.imwrite(os.path.join('test_output', 'input_with_prediction_overlay.jpg'), overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 2 continued: Try Stuff**\n",
    "\n",
    "Use your code above to try different architectures. Make sure to use early stopping! Try adding Dropout and BatchNorm, try different learning rates. How do they affect training and validation performance? \n",
    "\n",
    " **Summarize your observations in a paragraph below:**\n",
    " Since we have a large dataset, we decided to try this with only 10% of the data (1000 images). We found that deeper networks made training way more time consuming without giving us much benefit in performance of the final model, and we also found that Dropout did help with overfitting and slightly improved inference of our model, but only when we trained the model on a larger portion of the dataset, so we added Dropout. We also found that BatchNorm did not make a big difference. We found that the best learning rate was in the range of 0.001 to 0.004. We also implemented early stopping by keeping track of each epoch's loss on validation and stopping if the loss is worse than the previous 5 epochs. We hope to use the extension to mess around with different things more, since our biggest limitation has been time to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**\n",
    "\n",
    "As you can see, hyperparameter optimization can be tedious. In class we used [optuna](https://optuna.org/#code_examples) to automate the process. Your next task is to wrap your code from Step 2 into an objective which you can then optimize with optuna. Under the [code exaples](https://optuna.org/#code_examples) there is a tab *PyTorch* which should be helpful as it provides a minimal example on how to wrap PyTorch code inside an objective.\n",
    "\n",
    "**Important: Make sure the model is evaluated on a validation set, not the training data!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584d1593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:10:14,450] A new study created in memory with name: no-name-7c3acc29-8e2a-4486-8052-ecc30c686499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_350693/3962289433.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Training Loss: 457.6011 | Binary Loss: 457.6011\n",
      "Validation Loss: 1284.1334 | Binary Loss: 1284.1334\n",
      "Epoch 2/15\n",
      "Training Loss: 185.2092 | Binary Loss: 185.2092\n",
      "Validation Loss: 1125.5753 | Binary Loss: 1125.5753\n",
      "Epoch 3/15\n",
      "Training Loss: 119.3285 | Binary Loss: 119.3285\n",
      "Validation Loss: 1064.7070 | Binary Loss: 1064.7070\n",
      "Epoch 4/15\n",
      "Training Loss: 110.8861 | Binary Loss: 110.8861\n",
      "Validation Loss: 830.6626 | Binary Loss: 830.6626\n",
      "Epoch 5/15\n",
      "Training Loss: 102.8825 | Binary Loss: 102.8825\n",
      "Validation Loss: 791.7606 | Binary Loss: 791.7606\n",
      "Epoch 6/15\n",
      "Training Loss: 93.8583 | Binary Loss: 93.8583\n",
      "Validation Loss: 765.5646 | Binary Loss: 765.5646\n",
      "Epoch 7/15\n",
      "Training Loss: 91.0717 | Binary Loss: 91.0717\n",
      "Validation Loss: 744.3363 | Binary Loss: 744.3363\n",
      "Epoch 8/15\n",
      "Training Loss: 89.0643 | Binary Loss: 89.0643\n",
      "Validation Loss: 734.6345 | Binary Loss: 734.6345\n",
      "Epoch 9/15\n",
      "Training Loss: 88.4783 | Binary Loss: 88.4783\n",
      "Validation Loss: 729.9181 | Binary Loss: 729.9181\n",
      "Epoch 10/15\n",
      "Training Loss: 87.8365 | Binary Loss: 87.8365\n",
      "Validation Loss: 724.2953 | Binary Loss: 724.2953\n",
      "Epoch 11/15\n",
      "Training Loss: 87.6130 | Binary Loss: 87.6130\n",
      "Validation Loss: 723.6478 | Binary Loss: 723.6478\n",
      "Epoch 12/15\n",
      "Training Loss: 87.4617 | Binary Loss: 87.4617\n",
      "Validation Loss: 722.8901 | Binary Loss: 722.8901\n",
      "Epoch 13/15\n",
      "Training Loss: 87.0013 | Binary Loss: 87.0013\n",
      "Validation Loss: 722.0716 | Binary Loss: 722.0716\n",
      "Epoch 14/15\n",
      "Training Loss: 87.2419 | Binary Loss: 87.2419\n",
      "Validation Loss: 721.2719 | Binary Loss: 721.2719\n",
      "Epoch 15/15\n",
      "Training Loss: 87.1044 | Binary Loss: 87.1044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:15:39,674] Trial 0 finished with value: 720.5514109134674 and parameters: {'lr': 0.0028488698756383777, 'batch_size': 32, 'optimizer': 'Adam'}. Best is trial 0 with value: 720.5514109134674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 720.5514 | Binary Loss: 720.5514\n",
      "Epoch 1/15\n",
      "Training Loss: 697.4196 | Binary Loss: 697.4196\n",
      "Validation Loss: 5242.0920 | Binary Loss: 5242.0920\n",
      "Epoch 2/15\n",
      "Training Loss: 568.8501 | Binary Loss: 568.8501\n",
      "Validation Loss: 3974.0038 | Binary Loss: 3974.0038\n",
      "Epoch 3/15\n",
      "Training Loss: 424.5943 | Binary Loss: 424.5943\n",
      "Validation Loss: 2933.6669 | Binary Loss: 2933.6669\n",
      "Epoch 4/15\n",
      "Training Loss: 315.5001 | Binary Loss: 315.5001\n",
      "Validation Loss: 2222.8325 | Binary Loss: 2222.8325\n",
      "Epoch 5/15\n",
      "Training Loss: 243.1270 | Binary Loss: 243.1270\n",
      "Validation Loss: 1767.9155 | Binary Loss: 1767.9155\n",
      "Epoch 6/15\n",
      "Training Loss: 209.8668 | Binary Loss: 209.8668\n",
      "Validation Loss: 1735.3577 | Binary Loss: 1735.3577\n",
      "Epoch 7/15\n",
      "Training Loss: 206.2001 | Binary Loss: 206.2001\n",
      "Validation Loss: 1707.3242 | Binary Loss: 1707.3242\n",
      "Epoch 8/15\n",
      "Training Loss: 202.9805 | Binary Loss: 202.9805\n",
      "Validation Loss: 1681.8404 | Binary Loss: 1681.8404\n",
      "Epoch 9/15\n",
      "Training Loss: 200.0235 | Binary Loss: 200.0235\n",
      "Validation Loss: 1657.9914 | Binary Loss: 1657.9914\n",
      "Epoch 10/15\n",
      "Training Loss: 197.2256 | Binary Loss: 197.2256\n",
      "Validation Loss: 1635.3114 | Binary Loss: 1635.3114\n",
      "Epoch 11/15\n",
      "Training Loss: 195.4624 | Binary Loss: 195.4624\n",
      "Validation Loss: 1633.1139 | Binary Loss: 1633.1139\n",
      "Epoch 12/15\n",
      "Training Loss: 195.1993 | Binary Loss: 195.1993\n",
      "Validation Loss: 1630.9487 | Binary Loss: 1630.9487\n",
      "Epoch 13/15\n",
      "Training Loss: 194.9395 | Binary Loss: 194.9395\n",
      "Validation Loss: 1628.8026 | Binary Loss: 1628.8026\n",
      "Epoch 14/15\n",
      "Training Loss: 194.6824 | Binary Loss: 194.6824\n",
      "Validation Loss: 1626.6701 | Binary Loss: 1626.6701\n",
      "Epoch 15/15\n",
      "Training Loss: 194.4270 | Binary Loss: 194.4270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:21:01,614] Trial 1 finished with value: 1624.5446063280106 and parameters: {'lr': 0.0007567340516525363, 'batch_size': 16, 'optimizer': 'SGD'}. Best is trial 0 with value: 720.5514109134674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1624.5446 | Binary Loss: 1624.5446\n",
      "Epoch 1/15\n",
      "Training Loss: 741.5212 | Binary Loss: 741.5212\n",
      "Validation Loss: 6200.9681 | Binary Loss: 6200.9681\n",
      "Epoch 2/15\n",
      "Training Loss: 740.7170 | Binary Loss: 740.7170\n",
      "Validation Loss: 6194.3877 | Binary Loss: 6194.3877\n",
      "Epoch 3/15\n",
      "Training Loss: 739.9341 | Binary Loss: 739.9341\n",
      "Validation Loss: 6187.8660 | Binary Loss: 6187.8660\n",
      "Epoch 4/15\n",
      "Training Loss: 739.1718 | Binary Loss: 739.1718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:22:26,804] Trial 2 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 6181.4159 | Binary Loss: 6181.4159\n",
      "Epoch 1/15\n",
      "Training Loss: 570.6067 | Binary Loss: 570.6067\n",
      "Validation Loss: 4632.1980 | Binary Loss: 4632.1980\n",
      "Epoch 2/15\n",
      "Training Loss: 543.2350 | Binary Loss: 543.2350\n",
      "Validation Loss: 4280.1268 | Binary Loss: 4280.1268\n",
      "Epoch 3/15\n",
      "Training Loss: 497.8413 | Binary Loss: 497.8413\n",
      "Validation Loss: 3848.6661 | Binary Loss: 3848.6661\n",
      "Epoch 4/15\n",
      "Training Loss: 445.5739 | Binary Loss: 445.5739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:23:51,335] Trial 3 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3409.5172 | Binary Loss: 3409.5172\n",
      "Epoch 1/15\n",
      "Training Loss: 737.2841 | Binary Loss: 737.2841\n",
      "Validation Loss: 5970.7808 | Binary Loss: 5970.7808\n",
      "Epoch 2/15\n",
      "Training Loss: 713.3460 | Binary Loss: 713.3460\n",
      "Validation Loss: 5354.3066 | Binary Loss: 5354.3066\n",
      "Epoch 3/15\n",
      "Training Loss: 639.3289 | Binary Loss: 639.3289\n",
      "Validation Loss: 3838.0700 | Binary Loss: 3838.0700\n",
      "Epoch 4/15\n",
      "Training Loss: 458.8314 | Binary Loss: 458.8314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:25:17,939] Trial 4 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2267.2305 | Binary Loss: 2267.2305\n",
      "Epoch 1/15\n",
      "Training Loss: 703.7047 | Binary Loss: 703.7047\n",
      "Validation Loss: 5887.3589 | Binary Loss: 5887.3589\n",
      "Epoch 2/15\n",
      "Training Loss: 703.2767 | Binary Loss: 703.2767\n",
      "Validation Loss: 5883.7889 | Binary Loss: 5883.7889\n",
      "Epoch 3/15\n",
      "Training Loss: 702.8518 | Binary Loss: 702.8518\n",
      "Validation Loss: 5880.2069 | Binary Loss: 5880.2069\n",
      "Epoch 4/15\n",
      "Training Loss: 702.4237 | Binary Loss: 702.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:26:45,106] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5876.5771 | Binary Loss: 5876.5771\n",
      "Epoch 1/15\n",
      "Training Loss: 721.9696 | Binary Loss: 721.9696\n",
      "Validation Loss: 5521.2736 | Binary Loss: 5521.2736\n",
      "Epoch 2/15\n",
      "Training Loss: 548.6885 | Binary Loss: 548.6885\n",
      "Validation Loss: 2701.5889 | Binary Loss: 2701.5889\n",
      "Epoch 3/15\n",
      "Training Loss: 233.4089 | Binary Loss: 233.4089\n",
      "Validation Loss: 1578.5329 | Binary Loss: 1578.5329\n",
      "Epoch 4/15\n",
      "Training Loss: 199.0376 | Binary Loss: 199.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:28:10,156] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1588.5721 | Binary Loss: 1588.5721\n",
      "Epoch 1/15\n",
      "Training Loss: 731.7396 | Binary Loss: 731.7396\n",
      "Validation Loss: 6032.6644 | Binary Loss: 6032.6644\n",
      "Epoch 2/15\n",
      "Training Loss: 714.0469 | Binary Loss: 714.0469\n",
      "Validation Loss: 5799.7338 | Binary Loss: 5799.7338\n",
      "Epoch 3/15\n",
      "Training Loss: 683.6397 | Binary Loss: 683.6397\n",
      "Validation Loss: 5499.6481 | Binary Loss: 5499.6481\n",
      "Epoch 4/15\n",
      "Training Loss: 646.6267 | Binary Loss: 646.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:29:34,975] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5172.4071 | Binary Loss: 5172.4071\n",
      "Epoch 1/15\n",
      "Training Loss: 649.5221 | Binary Loss: 649.5221\n",
      "Validation Loss: 5429.1556 | Binary Loss: 5429.1556\n",
      "Epoch 2/15\n",
      "Training Loss: 647.8131 | Binary Loss: 647.8131\n",
      "Validation Loss: 5409.0900 | Binary Loss: 5409.0900\n",
      "Epoch 3/15\n",
      "Training Loss: 645.1627 | Binary Loss: 645.1627\n",
      "Validation Loss: 5384.2119 | Binary Loss: 5384.2119\n",
      "Epoch 4/15\n",
      "Training Loss: 642.0760 | Binary Loss: 642.0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:30:59,951] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5357.1679 | Binary Loss: 5357.1679\n",
      "Epoch 1/15\n",
      "Training Loss: 693.7491 | Binary Loss: 693.7491\n",
      "Validation Loss: 5798.0105 | Binary Loss: 5798.0105\n",
      "Epoch 2/15\n",
      "Training Loss: 692.0781 | Binary Loss: 692.0781\n",
      "Validation Loss: 5775.6475 | Binary Loss: 5775.6475\n",
      "Epoch 3/15\n",
      "Training Loss: 689.1305 | Binary Loss: 689.1305\n",
      "Validation Loss: 5745.5144 | Binary Loss: 5745.5144\n",
      "Epoch 4/15\n",
      "Training Loss: 685.3521 | Binary Loss: 685.3521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:32:27,003] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5710.4286 | Binary Loss: 5710.4286\n",
      "Epoch 1/15\n",
      "Training Loss: 729.3682 | Binary Loss: 729.3682\n",
      "Validation Loss: 4226.1427 | Binary Loss: 4226.1427\n",
      "Epoch 2/15\n",
      "Training Loss: 505.4047 | Binary Loss: 505.4047\n",
      "Validation Loss: 1764.4432 | Binary Loss: 1764.4432\n",
      "Epoch 3/15\n",
      "Training Loss: 210.2749 | Binary Loss: 210.2749\n",
      "Validation Loss: 2477.3511 | Binary Loss: 2477.3511\n",
      "Epoch 4/15\n",
      "Training Loss: 295.8389 | Binary Loss: 295.8389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:33:54,168] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1735.7565 | Binary Loss: 1735.7565\n",
      "Epoch 1/15\n",
      "Training Loss: 597.6442 | Binary Loss: 597.6442\n",
      "Validation Loss: 4045.5851 | Binary Loss: 4045.5851\n",
      "Epoch 2/15\n",
      "Training Loss: 406.9542 | Binary Loss: 406.9542\n",
      "Validation Loss: 2473.8994 | Binary Loss: 2473.8994\n",
      "Epoch 3/15\n",
      "Training Loss: 248.6470 | Binary Loss: 248.6470\n",
      "Validation Loss: 1574.5388 | Binary Loss: 1574.5388\n",
      "Epoch 4/15\n",
      "Training Loss: 166.2611 | Binary Loss: 166.2611\n",
      "Validation Loss: 1159.6640 | Binary Loss: 1159.6640\n",
      "Epoch 5/15\n",
      "Training Loss: 128.8382 | Binary Loss: 128.8382\n",
      "Validation Loss: 970.4685 | Binary Loss: 970.4685\n",
      "Epoch 6/15\n",
      "Training Loss: 115.6946 | Binary Loss: 115.6946\n",
      "Validation Loss: 959.7126 | Binary Loss: 959.7126\n",
      "Epoch 7/15\n",
      "Training Loss: 114.5466 | Binary Loss: 114.5466\n",
      "Validation Loss: 951.5045 | Binary Loss: 951.5045\n",
      "Epoch 8/15\n",
      "Training Loss: 113.6343 | Binary Loss: 113.6343\n",
      "Validation Loss: 944.6121 | Binary Loss: 944.6121\n",
      "Epoch 9/15\n",
      "Training Loss: 112.8501 | Binary Loss: 112.8501\n",
      "Validation Loss: 938.4412 | Binary Loss: 938.4412\n",
      "Epoch 10/15\n",
      "Training Loss: 112.1363 | Binary Loss: 112.1363\n",
      "Validation Loss: 932.7136 | Binary Loss: 932.7136\n",
      "Epoch 11/15\n",
      "Training Loss: 111.6891 | Binary Loss: 111.6891\n",
      "Validation Loss: 932.1670 | Binary Loss: 932.1670\n",
      "Epoch 12/15\n",
      "Training Loss: 111.6253 | Binary Loss: 111.6253\n",
      "Validation Loss: 931.6327 | Binary Loss: 931.6327\n",
      "Epoch 13/15\n",
      "Training Loss: 111.5614 | Binary Loss: 111.5614\n",
      "Validation Loss: 931.1052 | Binary Loss: 931.1052\n",
      "Epoch 14/15\n",
      "Training Loss: 111.4983 | Binary Loss: 111.4983\n",
      "Validation Loss: 930.5816 | Binary Loss: 930.5816\n",
      "Epoch 15/15\n",
      "Training Loss: 111.4364 | Binary Loss: 111.4364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:39:21,937] Trial 11 finished with value: 930.0603764057159 and parameters: {'lr': 0.0017284707129619876, 'batch_size': 16, 'optimizer': 'SGD'}. Best is trial 0 with value: 720.5514109134674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 930.0604 | Binary Loss: 930.0604\n",
      "Epoch 1/15\n",
      "Training Loss: 832.4450 | Binary Loss: 832.4450\n",
      "Validation Loss: 6714.7458 | Binary Loss: 6714.7458\n",
      "Epoch 2/15\n",
      "Training Loss: 791.0397 | Binary Loss: 791.0397\n",
      "Validation Loss: 6138.7287 | Binary Loss: 6138.7287\n",
      "Epoch 3/15\n",
      "Training Loss: 718.1548 | Binary Loss: 718.1548\n",
      "Validation Loss: 5404.5337 | Binary Loss: 5404.5337\n",
      "Epoch 4/15\n",
      "Training Loss: 628.9182 | Binary Loss: 628.9182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:40:49,934] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4623.2462 | Binary Loss: 4623.2462\n",
      "Epoch 1/15\n",
      "Training Loss: 342.0801 | Binary Loss: 342.0801\n",
      "Validation Loss: 1319.6030 | Binary Loss: 1319.6030\n",
      "Epoch 2/15\n",
      "Training Loss: 120.1376 | Binary Loss: 120.1376\n",
      "Validation Loss: 839.7959 | Binary Loss: 839.7959\n",
      "Epoch 3/15\n",
      "Training Loss: 98.7392 | Binary Loss: 98.7392\n",
      "Validation Loss: 747.7483 | Binary Loss: 747.7483\n",
      "Epoch 4/15\n",
      "Training Loss: 90.4433 | Binary Loss: 90.4433\n",
      "Validation Loss: 731.6487 | Binary Loss: 731.6487\n",
      "Epoch 5/15\n",
      "Training Loss: 88.1318 | Binary Loss: 88.1318\n",
      "Validation Loss: 711.5677 | Binary Loss: 711.5677\n",
      "Epoch 6/15\n",
      "Training Loss: 85.4540 | Binary Loss: 85.4540\n",
      "Validation Loss: 706.6779 | Binary Loss: 706.6779\n",
      "Epoch 7/15\n",
      "Training Loss: 84.9096 | Binary Loss: 84.9096\n",
      "Validation Loss: 697.1443 | Binary Loss: 697.1443\n",
      "Epoch 8/15\n",
      "Training Loss: 84.1064 | Binary Loss: 84.1064\n",
      "Validation Loss: 694.4857 | Binary Loss: 694.4857\n",
      "Epoch 9/15\n",
      "Training Loss: 84.0292 | Binary Loss: 84.0292\n",
      "Validation Loss: 692.4810 | Binary Loss: 692.4810\n",
      "Epoch 10/15\n",
      "Training Loss: 83.6618 | Binary Loss: 83.6618\n",
      "Validation Loss: 690.5733 | Binary Loss: 690.5733\n",
      "Epoch 11/15\n",
      "Training Loss: 83.8311 | Binary Loss: 83.8311\n",
      "Validation Loss: 690.1423 | Binary Loss: 690.1423\n",
      "Epoch 12/15\n",
      "Training Loss: 83.5463 | Binary Loss: 83.5463\n",
      "Validation Loss: 689.9054 | Binary Loss: 689.9054\n",
      "Epoch 13/15\n",
      "Training Loss: 83.4478 | Binary Loss: 83.4478\n",
      "Validation Loss: 689.7707 | Binary Loss: 689.7707\n",
      "Epoch 14/15\n",
      "Training Loss: 83.1943 | Binary Loss: 83.1943\n",
      "Validation Loss: 689.5083 | Binary Loss: 689.5083\n",
      "Epoch 15/15\n",
      "Training Loss: 83.2867 | Binary Loss: 83.2867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:46:15,097] Trial 13 finished with value: 689.1264949440956 and parameters: {'lr': 0.0072306956704766895, 'batch_size': 16, 'optimizer': 'Adam'}. Best is trial 13 with value: 689.1264949440956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 689.1265 | Binary Loss: 689.1265\n",
      "Epoch 1/15\n",
      "Training Loss: 383.3054 | Binary Loss: 383.3054\n",
      "Validation Loss: 1230.5810 | Binary Loss: 1230.5810\n",
      "Epoch 2/15\n",
      "Training Loss: 127.2585 | Binary Loss: 127.2585\n",
      "Validation Loss: 961.0655 | Binary Loss: 961.0655\n",
      "Epoch 3/15\n",
      "Training Loss: 105.5372 | Binary Loss: 105.5372\n",
      "Validation Loss: 801.7987 | Binary Loss: 801.7987\n",
      "Epoch 4/15\n",
      "Training Loss: 99.5582 | Binary Loss: 99.5582\n",
      "Validation Loss: 752.7385 | Binary Loss: 752.7385\n",
      "Epoch 5/15\n",
      "Training Loss: 95.0911 | Binary Loss: 95.0911\n",
      "Validation Loss: 791.4991 | Binary Loss: 791.4991\n",
      "Epoch 6/15\n",
      "Training Loss: 95.4446 | Binary Loss: 95.4446\n",
      "Validation Loss: 755.8560 | Binary Loss: 755.8560\n",
      "Epoch 7/15\n",
      "Training Loss: 89.4965 | Binary Loss: 89.4965\n",
      "Validation Loss: 720.0071 | Binary Loss: 720.0071\n",
      "Epoch 8/15\n",
      "Training Loss: 87.9359 | Binary Loss: 87.9359\n",
      "Validation Loss: 737.2969 | Binary Loss: 737.2969\n",
      "Epoch 9/15\n",
      "Training Loss: 88.6393 | Binary Loss: 88.6393\n",
      "Validation Loss: 719.3614 | Binary Loss: 719.3614\n",
      "Epoch 10/15\n",
      "Training Loss: 86.6270 | Binary Loss: 86.6270\n",
      "Validation Loss: 714.9551 | Binary Loss: 714.9551\n",
      "Epoch 11/15\n",
      "Training Loss: 86.3889 | Binary Loss: 86.3889\n",
      "Validation Loss: 715.1619 | Binary Loss: 715.1619\n",
      "Epoch 12/15\n",
      "Training Loss: 86.3477 | Binary Loss: 86.3477\n",
      "Validation Loss: 714.6063 | Binary Loss: 714.6063\n",
      "Epoch 13/15\n",
      "Training Loss: 86.1631 | Binary Loss: 86.1631\n",
      "Validation Loss: 713.4600 | Binary Loss: 713.4600\n",
      "Epoch 14/15\n",
      "Training Loss: 86.3063 | Binary Loss: 86.3063\n",
      "Validation Loss: 712.0153 | Binary Loss: 712.0153\n",
      "Epoch 15/15\n",
      "Training Loss: 86.1964 | Binary Loss: 86.1964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:51:40,721] Trial 14 finished with value: 710.9592831134796 and parameters: {'lr': 0.008417221799649469, 'batch_size': 32, 'optimizer': 'Adam'}. Best is trial 13 with value: 689.1264949440956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 710.9593 | Binary Loss: 710.9593\n",
      "Epoch 1/15\n",
      "Training Loss: 350.3881 | Binary Loss: 350.3881\n",
      "Validation Loss: 1130.0716 | Binary Loss: 1130.0716\n",
      "Epoch 2/15\n",
      "Training Loss: 129.6255 | Binary Loss: 129.6255\n",
      "Validation Loss: 1008.7328 | Binary Loss: 1008.7328\n",
      "Epoch 3/15\n",
      "Training Loss: 111.9180 | Binary Loss: 111.9180\n",
      "Validation Loss: 784.2329 | Binary Loss: 784.2329\n",
      "Epoch 4/15\n",
      "Training Loss: 92.5128 | Binary Loss: 92.5128\n",
      "Validation Loss: 709.8897 | Binary Loss: 709.8897\n",
      "Epoch 5/15\n",
      "Training Loss: 83.8103 | Binary Loss: 83.8103\n",
      "Validation Loss: 680.5921 | Binary Loss: 680.5921\n",
      "Epoch 6/15\n",
      "Training Loss: 81.9260 | Binary Loss: 81.9260\n",
      "Validation Loss: 669.8806 | Binary Loss: 669.8806\n",
      "Epoch 7/15\n",
      "Training Loss: 80.7326 | Binary Loss: 80.7326\n",
      "Validation Loss: 666.5199 | Binary Loss: 666.5199\n",
      "Epoch 8/15\n",
      "Training Loss: 80.6233 | Binary Loss: 80.6233\n",
      "Validation Loss: 664.6921 | Binary Loss: 664.6921\n",
      "Epoch 9/15\n",
      "Training Loss: 80.5440 | Binary Loss: 80.5440\n",
      "Validation Loss: 663.0781 | Binary Loss: 663.0781\n",
      "Epoch 10/15\n",
      "Training Loss: 80.1141 | Binary Loss: 80.1141\n",
      "Validation Loss: 660.0218 | Binary Loss: 660.0218\n",
      "Epoch 11/15\n",
      "Training Loss: 79.9404 | Binary Loss: 79.9404\n",
      "Validation Loss: 659.7781 | Binary Loss: 659.7781\n",
      "Epoch 12/15\n",
      "Training Loss: 79.8567 | Binary Loss: 79.8567\n",
      "Validation Loss: 659.6499 | Binary Loss: 659.6499\n",
      "Epoch 13/15\n",
      "Training Loss: 79.9287 | Binary Loss: 79.9287\n",
      "Validation Loss: 659.5623 | Binary Loss: 659.5623\n",
      "Epoch 14/15\n",
      "Training Loss: 79.8609 | Binary Loss: 79.8609\n",
      "Validation Loss: 659.4740 | Binary Loss: 659.4740\n",
      "Epoch 15/15\n",
      "Training Loss: 79.6337 | Binary Loss: 79.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:57:05,409] Trial 15 finished with value: 659.2741931676865 and parameters: {'lr': 0.009405886808874463, 'batch_size': 16, 'optimizer': 'Adam'}. Best is trial 15 with value: 659.2741931676865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 659.2742 | Binary Loss: 659.2742\n",
      "Epoch 1/15\n",
      "Training Loss: 646.1021 | Binary Loss: 646.1021\n",
      "Validation Loss: 5358.5081 | Binary Loss: 5358.5081\n",
      "Epoch 2/15\n",
      "Training Loss: 634.4232 | Binary Loss: 634.4232\n",
      "Validation Loss: 5203.9114 | Binary Loss: 5203.9114\n",
      "Epoch 3/15\n",
      "Training Loss: 608.8526 | Binary Loss: 608.8526\n",
      "Validation Loss: 4861.6639 | Binary Loss: 4861.6639\n",
      "Epoch 4/15\n",
      "Training Loss: 555.7357 | Binary Loss: 555.7357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 22:58:32,566] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4251.4456 | Binary Loss: 4251.4456\n",
      "Epoch 1/15\n",
      "Training Loss: 251.1440 | Binary Loss: 251.1440\n",
      "Validation Loss: 795.3431 | Binary Loss: 795.3431\n",
      "Epoch 2/15\n",
      "Training Loss: 96.6996 | Binary Loss: 96.6996\n",
      "Validation Loss: 780.7080 | Binary Loss: 780.7080\n",
      "Epoch 3/15\n",
      "Training Loss: 90.3943 | Binary Loss: 90.3943\n",
      "Validation Loss: 699.4892 | Binary Loss: 699.4892\n",
      "Epoch 4/15\n",
      "Training Loss: 84.1657 | Binary Loss: 84.1657\n",
      "Validation Loss: 704.7021 | Binary Loss: 704.7021\n",
      "Epoch 5/15\n",
      "Training Loss: 84.6999 | Binary Loss: 84.6999\n",
      "Validation Loss: 663.3749 | Binary Loss: 663.3749\n",
      "Epoch 6/15\n",
      "Training Loss: 80.2912 | Binary Loss: 80.2912\n",
      "Validation Loss: 655.5919 | Binary Loss: 655.5919\n",
      "Epoch 7/15\n",
      "Training Loss: 79.5912 | Binary Loss: 79.5912\n",
      "Validation Loss: 660.5322 | Binary Loss: 660.5322\n",
      "Epoch 8/15\n",
      "Training Loss: 79.3293 | Binary Loss: 79.3293\n",
      "Validation Loss: 648.9534 | Binary Loss: 648.9534\n",
      "Epoch 9/15\n",
      "Training Loss: 78.8233 | Binary Loss: 78.8233\n",
      "Validation Loss: 647.2829 | Binary Loss: 647.2829\n",
      "Epoch 10/15\n",
      "Training Loss: 78.4751 | Binary Loss: 78.4751\n",
      "Validation Loss: 647.5475 | Binary Loss: 647.5475\n",
      "Epoch 11/15\n",
      "Training Loss: 78.4644 | Binary Loss: 78.4644\n",
      "Validation Loss: 646.7270 | Binary Loss: 646.7270\n",
      "Epoch 12/15\n",
      "Training Loss: 78.2673 | Binary Loss: 78.2673\n",
      "Validation Loss: 645.6575 | Binary Loss: 645.6575\n",
      "Epoch 13/15\n",
      "Training Loss: 78.1759 | Binary Loss: 78.1759\n",
      "Validation Loss: 644.9242 | Binary Loss: 644.9242\n",
      "Epoch 14/15\n",
      "Training Loss: 78.0683 | Binary Loss: 78.0683\n",
      "Validation Loss: 644.5029 | Binary Loss: 644.5029\n",
      "Epoch 15/15\n",
      "Training Loss: 78.2353 | Binary Loss: 78.2353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 23:04:00,111] Trial 17 finished with value: 644.1527051925659 and parameters: {'lr': 0.007328191710575833, 'batch_size': 16, 'optimizer': 'Adam'}. Best is trial 17 with value: 644.1527051925659.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 644.1527 | Binary Loss: 644.1527\n",
      "Epoch 1/15\n",
      "Training Loss: 599.3834 | Binary Loss: 599.3834\n",
      "Validation Loss: 2262.5526 | Binary Loss: 2262.5526\n",
      "Epoch 2/15\n",
      "Training Loss: 202.0865 | Binary Loss: 202.0865\n",
      "Validation Loss: 1655.2545 | Binary Loss: 1655.2545\n",
      "Epoch 3/15\n",
      "Training Loss: 152.5385 | Binary Loss: 152.5385\n",
      "Validation Loss: 1064.3509 | Binary Loss: 1064.3509\n",
      "Epoch 4/15\n",
      "Training Loss: 122.5781 | Binary Loss: 122.5781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 23:05:27,300] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 840.3817 | Binary Loss: 840.3817\n",
      "Epoch 1/15\n",
      "Training Loss: 462.1770 | Binary Loss: 462.1770\n",
      "Validation Loss: 1379.7782 | Binary Loss: 1379.7782\n",
      "Epoch 2/15\n",
      "Training Loss: 137.2747 | Binary Loss: 137.2747\n",
      "Validation Loss: 894.3344 | Binary Loss: 894.3344\n",
      "Epoch 3/15\n",
      "Training Loss: 109.0129 | Binary Loss: 109.0129\n",
      "Validation Loss: 827.5910 | Binary Loss: 827.5910\n",
      "Epoch 4/15\n",
      "Training Loss: 97.5343 | Binary Loss: 97.5343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 23:06:54,687] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 773.1251 | Binary Loss: 773.1251\n",
      "Best hyperparameters: {'lr': 0.007328191710575833, 'batch_size': 16, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna.exceptions\n",
    "\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=True, optuna=True)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "\n",
    "# Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LaneLines().to(device)\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    # loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "    num_epochs = 15\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"Training Loss: {train_loss:.4f} | Binary Loss: {train_loss_b:.4f}\")\n",
    "\n",
    "        val_loss, val_loss_b = test_loop(model, val_dataloader, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=3))\n",
    "\n",
    "# Optimize the objective function\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print out the best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b007",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**\n",
    "\n",
    "The hyperparameter search was somewhat helpful, but we think we weren’t able to harness a lot of the potential benefits due to the significant training times when running the Optuna trials – even with only using a small percentage (5-10%) of the data. While increasing the number of trials did yield hyperparameters that improved results, we could not increase the number of trials by a significant amount. Additionally, we found the pruning feature on the Optuna website and found it to be useful in cutting off less promising trials early, which allowed us to bump up the number of trials. We plan to look into potentially using cloud compute so we can run more, longer trials for the hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54312a3",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n",
    "\n",
    "Now that you have found a good hyperparameter setting the validation set is no longer needed. The last step is to combine the training and validation set into a combined training set and retrain the model under the best parameter setting found. Report your final loss on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d1a3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_lr = study.best_params['lr']\n",
    "optuna_batch_size = study.best_params['batch_size']\n",
    "optuna_optimizer = study.best_params['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a3c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Training Loss: 1879.4857931137085 | Binary Loss: 1879.4857931137085\n",
      "Validation Loss: 604.4004 | Binary Loss: 604.4004\n",
      "Epoch 2/100\n",
      "Training Loss: 1405.152413368225 | Binary Loss: 1405.152413368225\n",
      "Validation Loss: 564.0801 | Binary Loss: 564.0801\n",
      "Epoch 3/100\n",
      "Training Loss: 1327.1682257652283 | Binary Loss: 1327.1682257652283\n",
      "Validation Loss: 531.2248 | Binary Loss: 531.2248\n",
      "Epoch 4/100\n",
      "Training Loss: 1260.9797005653381 | Binary Loss: 1260.9797005653381\n",
      "Validation Loss: 508.2955 | Binary Loss: 508.2955\n",
      "Epoch 5/100\n",
      "Training Loss: 1218.2152633666992 | Binary Loss: 1218.2152633666992\n",
      "Validation Loss: 480.7238 | Binary Loss: 480.7238\n",
      "Epoch 6/100\n",
      "Training Loss: 1162.5182104110718 | Binary Loss: 1162.5182104110718\n",
      "Validation Loss: 469.5759 | Binary Loss: 469.5759\n",
      "Epoch 7/100\n",
      "Training Loss: 1126.847858428955 | Binary Loss: 1126.847858428955\n",
      "Validation Loss: 458.5229 | Binary Loss: 458.5229\n",
      "Epoch 8/100\n",
      "Training Loss: 1112.384331703186 | Binary Loss: 1112.384331703186\n",
      "Validation Loss: 444.9471 | Binary Loss: 444.9471\n",
      "Epoch 9/100\n",
      "Training Loss: 1084.9497809410095 | Binary Loss: 1084.9497809410095\n",
      "Validation Loss: 442.7101 | Binary Loss: 442.7101\n",
      "Epoch 10/100\n",
      "Training Loss: 1064.9062461853027 | Binary Loss: 1064.9062461853027\n",
      "Validation Loss: 430.7434 | Binary Loss: 430.7434\n",
      "Epoch 11/100\n",
      "Training Loss: 1016.6384761333466 | Binary Loss: 1016.6384761333466\n",
      "Validation Loss: 418.5609 | Binary Loss: 418.5609\n",
      "Epoch 12/100\n",
      "Training Loss: 1003.7713389396667 | Binary Loss: 1003.7713389396667\n",
      "Validation Loss: 416.7241 | Binary Loss: 416.7241\n",
      "Epoch 13/100\n",
      "Training Loss: 999.085883140564 | Binary Loss: 999.085883140564\n",
      "Validation Loss: 412.5164 | Binary Loss: 412.5164\n",
      "Epoch 14/100\n",
      "Training Loss: 995.008261680603 | Binary Loss: 995.008261680603\n",
      "Validation Loss: 412.2918 | Binary Loss: 412.2918\n",
      "Epoch 15/100\n",
      "Training Loss: 991.2496304512024 | Binary Loss: 991.2496304512024\n",
      "Validation Loss: 409.4670 | Binary Loss: 409.4670\n",
      "Epoch 16/100\n",
      "Training Loss: 989.2933821678162 | Binary Loss: 989.2933821678162\n",
      "Validation Loss: 409.9065 | Binary Loss: 409.9065\n",
      "Epoch 17/100\n",
      "Training Loss: 985.0149574279785 | Binary Loss: 985.0149574279785\n",
      "Validation Loss: 406.2409 | Binary Loss: 406.2409\n",
      "Epoch 18/100\n",
      "Training Loss: 981.0783159732819 | Binary Loss: 981.0783159732819\n",
      "Validation Loss: 406.8029 | Binary Loss: 406.8029\n",
      "Epoch 19/100\n",
      "Training Loss: 979.2248153686523 | Binary Loss: 979.2248153686523\n",
      "Validation Loss: 405.2617 | Binary Loss: 405.2617\n",
      "Epoch 20/100\n",
      "Training Loss: 975.4011993408203 | Binary Loss: 975.4011993408203\n",
      "Validation Loss: 404.7172 | Binary Loss: 404.7172\n",
      "Epoch 21/100\n",
      "Training Loss: 966.3694996833801 | Binary Loss: 966.3694996833801\n",
      "Validation Loss: 400.9798 | Binary Loss: 400.9798\n",
      "Epoch 22/100\n",
      "Training Loss: 964.5091547966003 | Binary Loss: 964.5091547966003\n",
      "Validation Loss: 400.7989 | Binary Loss: 400.7989\n",
      "Epoch 23/100\n",
      "Training Loss: 963.2154703140259 | Binary Loss: 963.2154703140259\n",
      "Validation Loss: 400.5063 | Binary Loss: 400.5063\n",
      "Epoch 24/100\n",
      "Training Loss: 962.7980380058289 | Binary Loss: 962.7980380058289\n",
      "Validation Loss: 400.4534 | Binary Loss: 400.4534\n",
      "Epoch 25/100\n",
      "Training Loss: 962.6347343921661 | Binary Loss: 962.6347343921661\n",
      "Validation Loss: 399.9430 | Binary Loss: 399.9430\n",
      "Epoch 26/100\n",
      "Training Loss: 962.1467657089233 | Binary Loss: 962.1467657089233\n",
      "Validation Loss: 399.6276 | Binary Loss: 399.6276\n",
      "Epoch 27/100\n",
      "Training Loss: 960.7019424438477 | Binary Loss: 960.7019424438477\n",
      "Validation Loss: 399.6956 | Binary Loss: 399.6956\n",
      "Epoch 28/100\n",
      "Training Loss: 960.8403100967407 | Binary Loss: 960.8403100967407\n",
      "Validation Loss: 399.5121 | Binary Loss: 399.5121\n",
      "Epoch 29/100\n",
      "Training Loss: 959.9081091880798 | Binary Loss: 959.9081091880798\n",
      "Validation Loss: 399.0486 | Binary Loss: 399.0486\n",
      "Epoch 30/100\n",
      "Training Loss: 959.8144402503967 | Binary Loss: 959.8144402503967\n",
      "Validation Loss: 398.8705 | Binary Loss: 398.8705\n",
      "Epoch 31/100\n",
      "Training Loss: 958.4779953956604 | Binary Loss: 958.4779953956604\n",
      "Validation Loss: 398.8487 | Binary Loss: 398.8487\n",
      "Epoch 32/100\n",
      "Training Loss: 959.3998484611511 | Binary Loss: 959.3998484611511\n",
      "Validation Loss: 398.8638 | Binary Loss: 398.8638\n",
      "Epoch 33/100\n",
      "Training Loss: 957.285406589508 | Binary Loss: 957.285406589508\n",
      "Validation Loss: 398.8664 | Binary Loss: 398.8664\n",
      "Epoch 34/100\n",
      "Training Loss: 959.2408502101898 | Binary Loss: 959.2408502101898\n",
      "Validation Loss: 398.7724 | Binary Loss: 398.7724\n",
      "Epoch 35/100\n",
      "Training Loss: 958.3968505859375 | Binary Loss: 958.3968505859375\n",
      "Validation Loss: 398.8718 | Binary Loss: 398.8718\n",
      "Epoch 36/100\n",
      "Training Loss: 958.0125851631165 | Binary Loss: 958.0125851631165\n",
      "Validation Loss: 398.7863 | Binary Loss: 398.7863\n",
      "Epoch 37/100\n",
      "Training Loss: 958.1441304683685 | Binary Loss: 958.1441304683685\n",
      "Validation Loss: 398.7715 | Binary Loss: 398.7715\n",
      "Epoch 38/100\n",
      "Training Loss: 957.814254283905 | Binary Loss: 957.814254283905\n",
      "Validation Loss: 398.7160 | Binary Loss: 398.7160\n",
      "Epoch 39/100\n",
      "Training Loss: 958.8384871482849 | Binary Loss: 958.8384871482849\n",
      "Validation Loss: 398.7369 | Binary Loss: 398.7369\n",
      "Epoch 40/100\n",
      "Training Loss: 957.6619548797607 | Binary Loss: 957.6619548797607\n",
      "Validation Loss: 398.6734 | Binary Loss: 398.6734\n",
      "Epoch 41/100\n",
      "Training Loss: 957.8087816238403 | Binary Loss: 957.8087816238403\n",
      "Validation Loss: 398.6796 | Binary Loss: 398.6796\n",
      "Epoch 42/100\n",
      "Training Loss: 957.6520104408264 | Binary Loss: 957.6520104408264\n",
      "Validation Loss: 398.6625 | Binary Loss: 398.6625\n",
      "Epoch 43/100\n",
      "Training Loss: 957.660101890564 | Binary Loss: 957.660101890564\n",
      "Validation Loss: 398.6719 | Binary Loss: 398.6719\n",
      "Epoch 44/100\n",
      "Training Loss: 956.8836822509766 | Binary Loss: 956.8836822509766\n",
      "Validation Loss: 398.6768 | Binary Loss: 398.6768\n",
      "Epoch 45/100\n",
      "Training Loss: 957.3750371932983 | Binary Loss: 957.3750371932983\n",
      "Validation Loss: 398.6681 | Binary Loss: 398.6681\n",
      "Epoch 46/100\n",
      "Training Loss: 958.0739903450012 | Binary Loss: 958.0739903450012\n",
      "Validation Loss: 398.6516 | Binary Loss: 398.6516\n",
      "Epoch 47/100\n",
      "Training Loss: 959.0362622737885 | Binary Loss: 959.0362622737885\n",
      "Validation Loss: 398.6620 | Binary Loss: 398.6620\n",
      "Epoch 48/100\n",
      "Training Loss: 957.6674642562866 | Binary Loss: 957.6674642562866\n",
      "Validation Loss: 398.6498 | Binary Loss: 398.6498\n",
      "Epoch 49/100\n",
      "Training Loss: 958.1682391166687 | Binary Loss: 958.1682391166687\n",
      "Validation Loss: 398.6369 | Binary Loss: 398.6369\n",
      "Epoch 50/100\n",
      "Training Loss: 958.0804724693298 | Binary Loss: 958.0804724693298\n",
      "Validation Loss: 398.6410 | Binary Loss: 398.6410\n",
      "Epoch 51/100\n",
      "Training Loss: 957.5014429092407 | Binary Loss: 957.5014429092407\n",
      "Validation Loss: 398.6401 | Binary Loss: 398.6401\n",
      "Epoch 52/100\n",
      "Training Loss: 958.4540419578552 | Binary Loss: 958.4540419578552\n",
      "Validation Loss: 398.6402 | Binary Loss: 398.6402\n",
      "Epoch 53/100\n",
      "Training Loss: 957.2646949291229 | Binary Loss: 957.2646949291229\n",
      "Validation Loss: 398.6403 | Binary Loss: 398.6403\n",
      "Epoch 54/100\n",
      "Training Loss: 957.1656546592712 | Binary Loss: 957.1656546592712\n",
      "Validation Loss: 398.6382 | Binary Loss: 398.6382\n",
      "Epoch 55/100\n",
      "Training Loss: 957.9883470535278 | Binary Loss: 957.9883470535278\n",
      "Validation Loss: 398.6383 | Binary Loss: 398.6383\n",
      "Epoch 56/100\n",
      "Training Loss: 957.5020394325256 | Binary Loss: 957.5020394325256\n",
      "Validation Loss: 398.6381 | Binary Loss: 398.6381\n",
      "Epoch 57/100\n",
      "Training Loss: 957.711757183075 | Binary Loss: 957.711757183075\n",
      "Validation Loss: 398.6375 | Binary Loss: 398.6375\n",
      "Epoch 58/100\n",
      "Training Loss: 958.5630259513855 | Binary Loss: 958.5630259513855\n",
      "Validation Loss: 398.6367 | Binary Loss: 398.6367\n",
      "Epoch 59/100\n",
      "Training Loss: 957.4458079338074 | Binary Loss: 957.4458079338074\n",
      "Validation Loss: 398.6363 | Binary Loss: 398.6363\n",
      "Epoch 60/100\n",
      "Training Loss: 957.4815058708191 | Binary Loss: 957.4815058708191\n",
      "Validation Loss: 398.6369 | Binary Loss: 398.6369\n",
      "Epoch 61/100\n",
      "Training Loss: 957.9119148254395 | Binary Loss: 957.9119148254395\n",
      "Validation Loss: 398.6370 | Binary Loss: 398.6370\n",
      "Epoch 62/100\n",
      "Training Loss: 958.5494475364685 | Binary Loss: 958.5494475364685\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 63/100\n",
      "Training Loss: 958.9163465499878 | Binary Loss: 958.9163465499878\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 64/100\n",
      "Training Loss: 958.0416457653046 | Binary Loss: 958.0416457653046\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 65/100\n",
      "Training Loss: 957.513206243515 | Binary Loss: 957.513206243515\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 66/100\n",
      "Training Loss: 958.5706076622009 | Binary Loss: 958.5706076622009\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 67/100\n",
      "Training Loss: 958.1161389350891 | Binary Loss: 958.1161389350891\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 68/100\n",
      "Training Loss: 957.927318572998 | Binary Loss: 957.927318572998\n",
      "Validation Loss: 398.6371 | Binary Loss: 398.6371\n",
      "Epoch 69/100\n",
      "Training Loss: 958.6417555809021 | Binary Loss: 958.6417555809021\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 70/100\n",
      "Training Loss: 958.1138887405396 | Binary Loss: 958.1138887405396\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 71/100\n",
      "Training Loss: 957.1654133796692 | Binary Loss: 957.1654133796692\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 72/100\n",
      "Training Loss: 957.8480274677277 | Binary Loss: 957.8480274677277\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 73/100\n",
      "Training Loss: 957.8296709060669 | Binary Loss: 957.8296709060669\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 74/100\n",
      "Training Loss: 957.2758314609528 | Binary Loss: 957.2758314609528\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 75/100\n",
      "Training Loss: 957.1763706207275 | Binary Loss: 957.1763706207275\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 76/100\n",
      "Training Loss: 958.7701427936554 | Binary Loss: 958.7701427936554\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 77/100\n",
      "Training Loss: 958.7455384731293 | Binary Loss: 958.7455384731293\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 78/100\n",
      "Training Loss: 958.5385060310364 | Binary Loss: 958.5385060310364\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 79/100\n",
      "Training Loss: 957.0862755775452 | Binary Loss: 957.0862755775452\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 80/100\n",
      "Training Loss: 957.7689290046692 | Binary Loss: 957.7689290046692\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 81/100\n",
      "Training Loss: 958.003874540329 | Binary Loss: 958.003874540329\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 82/100\n",
      "Training Loss: 957.4278607368469 | Binary Loss: 957.4278607368469\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 83/100\n",
      "Training Loss: 957.3201942443848 | Binary Loss: 957.3201942443848\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 84/100\n",
      "Training Loss: 957.4225809574127 | Binary Loss: 957.4225809574127\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 85/100\n",
      "Training Loss: 957.5094141960144 | Binary Loss: 957.5094141960144\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 86/100\n",
      "Training Loss: 957.2252860069275 | Binary Loss: 957.2252860069275\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 87/100\n",
      "Training Loss: 958.3746137619019 | Binary Loss: 958.3746137619019\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 88/100\n",
      "Training Loss: 957.4495213031769 | Binary Loss: 957.4495213031769\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 89/100\n",
      "Training Loss: 957.6788396835327 | Binary Loss: 957.6788396835327\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 90/100\n",
      "Training Loss: 957.2353591918945 | Binary Loss: 957.2353591918945\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 91/100\n",
      "Training Loss: 958.1560823917389 | Binary Loss: 958.1560823917389\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 92/100\n",
      "Training Loss: 958.1172575950623 | Binary Loss: 958.1172575950623\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 93/100\n",
      "Training Loss: 957.6524209976196 | Binary Loss: 957.6524209976196\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 94/100\n",
      "Training Loss: 957.9860935211182 | Binary Loss: 957.9860935211182\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 95/100\n",
      "Training Loss: 958.6131510734558 | Binary Loss: 958.6131510734558\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 96/100\n",
      "Training Loss: 958.2453832626343 | Binary Loss: 958.2453832626343\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 97/100\n",
      "Training Loss: 957.1972143650055 | Binary Loss: 957.1972143650055\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 98/100\n",
      "Training Loss: 957.5225486755371 | Binary Loss: 957.5225486755371\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 99/100\n",
      "Training Loss: 958.1516966819763 | Binary Loss: 958.1516966819763\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n",
      "Epoch 100/100\n",
      "Training Loss: 957.3454031944275 | Binary Loss: 957.3454031944275\n",
      "Validation Loss: 398.6372 | Binary Loss: 398.6372\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_dataset = TusimpleData(train_dataset_file, transform=data_transforms['train'], target_transform=target_transforms, training=True)\n",
    "val_dataset = TusimpleData(val_dataset_file, transform=data_transforms['val'], target_transform=target_transforms, training=False)\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneLines().to(DEVICE)\n",
    "\n",
    "if optuna_optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=optuna_lr, momentum=0.9)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=optuna_lr)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=optuna_batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=optuna_batch_size, shuffle=False)\n",
    "num_epochs = 100\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_loss = float(\"inf\")\n",
    "losses = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_loss_b = train_loop(model, train_dataloader, optimizer, scheduler, DEVICE)\n",
    "    print(f\"Training Loss: {train_loss} | Binary Loss: {train_loss_b}\")\n",
    "    val_loss, val_loss_b = test_loop(model, val_dataloader, DEVICE)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Binary Loss: {val_loss_b:.4f}\")\n",
    "\n",
    "    losses[epoch] = val_loss\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_wts, \"best_model.pth\")\n",
    "\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# test the model\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_test_data(img_path, transform):\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "def test():\n",
    "    if not os.path.exists('test_output'):\n",
    "        os.mkdir('test_output')\n",
    "    img_path = '0001.png'\n",
    "    resize_height, resize_width = 256, 512\n",
    "    model_path = 'best_model.pth'\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_height, resize_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    model = LaneLines()\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    inp = load_test_data(img_path, data_transform).to(DEVICE)\n",
    "    inp = torch.unsqueeze(inp, dim=0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inp)\n",
    "    input_img = Image.open(img_path)\n",
    "    input_img = input_img.resize((resize_width, resize_height))\n",
    "    input_img = np.array(input_img)\n",
    "    binary_pred = outputs['binary_seg_pred']\n",
    "    binary_pred_np = binary_pred.detach().cpu().numpy()\n",
    "    overlay = input_img.copy()\n",
    "    overlay[binary_pred_np[0, 0, :, :] > 0] = [0, 0, 255]\n",
    "    cv2.imwrite(os.path.join('test_output', 'input_with_prediction_overlay.jpg'), overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 2 to Canvas. \n",
    "Happy Deep Learning! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
